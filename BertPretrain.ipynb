{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer, WordpieceTokenizer\n",
    "#from pytorch_pretrained_bert.modeling import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "from torch import Tensor\n",
    "from fastai.text import Tokenizer, Vocab\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import pdb\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "#from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import EarlyStopping\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s -   %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def log(message):\n",
    "    #outputs to Jupyter console\n",
    "    print('{} {}'.format(datetime.datetime.now(), message))\n",
    "    #outputs to file\n",
    "    logger.info(message)\n",
    "\n",
    "setup_file_logger('out.log')\n",
    "\n",
    "# for i in range(10000):\n",
    "#     log('Doing hard work here i=' + str(i))\n",
    "#     log('Taking a nap now...')\n",
    "#     time.sleep(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/gicgpu/Know-Your-Intent\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "print(pathlib.Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertPretrain.ipynb\t       out.log\t\tsemhash_pipeline.ipynb\r\n",
      "comprehensive_semhash_test.py  plots\t\ttable\r\n",
      "datasets\t\t       __pycache__\ttrain_test_split.py\r\n",
      "gitignore\t\t       pytorchtools.py\tupdated_semhash_pipeline.ipynb\r\n",
      "hashembedding\t\t       README.md\r\n",
      "intent_classification.ipynb    results.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/home/gicgpu/Know-Your-Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None' 'Make Update' 'Setup Printer' 'Software Recommendation' 'Shutdown Computer']\n"
     ]
    }
   ],
   "source": [
    "def load_label_list(trainset_path):\n",
    "    df=pd.read_csv(trainset_path, header=None, sep='\\t')\n",
    "    return df[1].unique()\n",
    "    \n",
    "label_list=load_label_list('./datasets/KL/AskUbuntu/train.csv')\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('/data/home/gicgpu/Know-Your-Intent/datasets/KL/AskUbuntu/')\n",
    "#DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "PATH=Path('/data/home/gicgpu/Know-Your-Intent/datasets/KL/AskUbuntu/')\n",
    "# PATH.mkdir(exist_ok=True)\n",
    "\n",
    "CLAS_DATA_PATH=PATH/'class/'\n",
    "#CLAS_DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# model_state_dict = None\n",
    "\n",
    "# BERT_PRETRAINED_PATH = Path('../trained_model/')\n",
    "BERT_PRETRAINED_PATH = 'bert-base-uncased'\n",
    "# BERT_PRETRAINED_PATH = Path('../../complaints/bert/pretrained-weights/cased_L-12_H-768_A-12/')\n",
    "# BERT_PRETRAINED_PATH = Path('../../complaints/bert/pretrained-weights/uncased_L-24_H-1024_A-16/')\n",
    "\n",
    "\n",
    "# BERT_FINETUNED_WEIGHTS = Path('../trained_model/toxic_comments')\n",
    "\n",
    "#PYTORCH_PRETRAINED_BERT_CACHE = BERT_PRETRAINED_PATH/'cache/'\n",
    "#PYTORCH_PRETRAINED_BERT_CACHE.mkdir(exist_ok=True)\n",
    "\n",
    "# output_model_file = os.path.join(BERT_FINETUNED_WEIGHTS, \"pytorch_model.bin\")\n",
    "\n",
    "# Load a trained model that you have fine-tuned\n",
    "# model_state_dict = torch.load('/data/home/gicgpu/sprint4/model/finetuned_on_scraped.bin') #(!!!) load finetuned model here\n",
    "# model_state_dict = torch.load(os.path.join('/data/home/gicgpu/sprint4/model/', \"bert27trainontrainthenVal.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": PATH,\n",
    "    \"task_name\": \"gic_bert\",\n",
    "    \"no_cuda\": False,\n",
    "    \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    \"output_dir\": CLAS_DATA_PATH,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"num_train_epochs\": 20.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "\n",
    "            loss = loss_fct(logits, labels.long())\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTextProcessor(DataProcessor):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = None\n",
    "    \n",
    "    \n",
    "    def get_train_examples(self, data_dir, size=-1):\n",
    "        filename = 'train.csv'\n",
    "#         filename = 'val.csv' ## (!!!) switch to train on val\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, filename)))\n",
    "        if size == -1:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename), header=None, sep='\\t')\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df, \"train\")\n",
    "        else:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename), header=None, sep='\\t')\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df.sample(size), \"train\")\n",
    "        \n",
    "    def get_dev_examples(self, data_dir, size=-1):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        filename = 'test.csv'\n",
    "        if size == -1:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename), header=None, sep='\\t')\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df, \"dev\")\n",
    "        else:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename), header=None, sep='\\t')\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df.sample(size), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        data_df = pd.read_csv(os.path.join(data_dir, data_file_name), header=None, sep='\\t')\n",
    "#         data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "        if size == -1:\n",
    "            return self._create_examples(data_df, \"test\")\n",
    "        else:\n",
    "            return self._create_examples(data_df.sample(size), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if self.labels == None:\n",
    "            df=pd.read_csv(os.path.join(data_dir, filename), header=None, sep='\\t')\n",
    "            self.labels = df[1].unique()\n",
    "#             self.labels = list(pd.read_csv(os.path.join(self.data_dir, \"classes.txt\"),header=None)[0].values)\n",
    "        return self.labels\n",
    "\n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, row) in enumerate(df.values):\n",
    "            guid = i # (!!!) change if any changes to dataframe\n",
    "            text_a = row[0] # (!!!) change if any changes to dataframe\n",
    "            if labels_available:\n",
    "                labels = row[1] # (!!!) change if any changes to dataframe\n",
    "            else:\n",
    "                labels = []\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "#     print(label_map)\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         try:\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "#             print(\"Success!\")\n",
    "#         except Exception as e:\n",
    "#             print(\"Failed index:\",ex_index)\n",
    "#             print(\"Failed ID:\",example.guid)\n",
    "#             print(\"Failed Text:\",example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "#         print('exxxample', example.labels)\n",
    "        labels_ids = label_map[example.labels]\n",
    "#         for label in example.labels:\n",
    "#             print(label)\n",
    "#             labels_ids.append(float(label_map[label]))\n",
    "\n",
    "#         label_id = label_map[example.label]\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    print(outputs.cpu() == labels.cpu().long())\n",
    "    return torch.sum(outputs.cpu() == labels.cpu().long())\n",
    "\n",
    "# def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True):\n",
    "#     \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "#     if sigmoid: y_pred = y_pred.sigmoid()\n",
    "#     print('dfdfdfdf', np.argmax(y_pred, axis=1), y_true, np.argmax(y_pred, axis=1)==)\n",
    "#     return (np.argmax(y_pred.cpu())==y_true.cpu().long).mean().item()\n",
    "# #     return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
    "\n",
    "\n",
    "def fbeta(y_pred:Tensor, y_true:Tensor, thresh:float=0.2, beta:float=2, eps:float=1e-9, sigmoid:bool=True):\n",
    "    \"Computes the f_beta between `preds` and `targets`\"\n",
    "    beta2 = beta ** 2\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "    y_pred = (y_pred>thresh).float()\n",
    "    y_true = y_true.float()\n",
    "    TP = (y_pred*y_true).sum(dim=1)\n",
    "    prec = TP/(y_pred.sum(dim=1)+eps)\n",
    "    rec = TP/(y_true.sum(dim=1)+eps)\n",
    "    res = (prec*rec)/(prec*beta2+rec+eps)*(1+beta2)\n",
    "    return res.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:30:23 - INFO - root -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "processors = {\n",
    "    \"gic_bert\": MultiLabelTextProcessor\n",
    "} # use default multiclass here instead lik\n",
    "\n",
    "# Setup GPU parameters\n",
    "\n",
    "if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "#     n_gpu = 1\n",
    "else:\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "    device = torch.device(\"cuda\", args['local_rank'])\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args['local_rank'] != -1), args['fp16']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['train_batch_size'] = int(args['train_batch_size'] / args['gradient_accumulation_steps'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = args['task_name'].lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = processors[task_name](args['data_dir'])\n",
    "label_list = load_label_list('./datasets/KL/AskUbuntu/train.csv')\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['None', 'Make Update', 'Setup Printer', 'Software Recommendation', 'Shutdown Computer'], dtype=object)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:30:25 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/gicgpu/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args['bert_model'], do_lower_case=args['do_lower_case'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:30:25 - INFO - root -   LOOKING AT /data/home/gicgpu/Know-Your-Intent/datasets/KL/AskUbuntu/train.csv\n"
     ]
    }
   ],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args['do_train']:\n",
    "    train_examples = processor.get_train_examples(args['full_data_dir'], size=args['train_size'])\n",
    "#     train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:30:26 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/gicgpu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/24/2019 11:30:26 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/gicgpu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpigtnf8lf\n",
      "04/24/2019 11:30:30 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/24/2019 11:30:33 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/24/2019 11:30:33 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "def get_model():\n",
    "#     pdb.set_trace()\n",
    "#     if model_state_dict:\n",
    "#         model = BertForMultiLabelSequenceClassification.from_pretrained(args['bert_model'], num_labels = num_labels, state_dict=model_state_dict)\n",
    "#     else:\n",
    "    model = BertForMultiLabelSequenceClassification.from_pretrained(args['bert_model'], num_labels = num_labels)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "if args['fp16']:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if args['local_rank'] != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, Optimizer\n",
    "\n",
    "class CyclicLR(object):\n",
    "    \"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `batch_step` should be called after a batch has been used for training.\n",
    "    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for eachparam groups.\n",
    "            Default: 0.001\n",
    "        max_lr (float or list): Upper boundaries in the cycle for\n",
    "            each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function. Default: 0.006\n",
    "        step_size (int): Number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch. Default: 2000\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        last_batch_iteration (int): The index of the last batch. Default: -1\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.CyclicLR(optimizer)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>         train_batch(...)\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "#         if not isinstance(optimizer, Optimizer):\n",
    "#             raise TypeError('{} is not an Optimizer'.format(\n",
    "#                 type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "if args['local_rank'] != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "if args['fp16']:\n",
    "    try:\n",
    "        from apex.optimizers import FP16_Optimizer\n",
    "        from apex.optimizers import FusedAdam\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                          lr=args['learning_rate'],\n",
    "                          bias_correction=False,\n",
    "                          max_grad_norm=1.0)\n",
    "    if args['loss_scale'] == 0:\n",
    "        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "    else:\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=args['loss_scale'])\n",
    "\n",
    "else:\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=args['learning_rate'],\n",
    "                         warmup=args['warmup_proportion'],\n",
    "                         t_total=t_total)\n",
    "\n",
    "scheduler = CyclicLR(optimizer, base_lr=2e-5, max_lr=5e-5, step_size=2500, last_batch_iteration=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Fn\n",
    "eval_examples = processor.get_dev_examples(args['data_dir'], size=args['val_size'])\n",
    "def eval():\n",
    "    args['output_dir'].mkdir(exist_ok=True)\n",
    "\n",
    "    \n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.float)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    all_logits = None\n",
    "    all_labels = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = label_ids.to('cpu').numpy()\n",
    "#         tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        if all_labels is None:\n",
    "            all_labels = label_ids.detach().cpu().numpy()\n",
    "        else:    \n",
    "            all_labels = np.concatenate((all_labels, label_ids.detach().cpu().numpy()), axis=0)\n",
    "        \n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    \n",
    "#     ROC-AUC calcualation\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "#     for i in range(num_labels):\n",
    "#         fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
    "#         roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "#     # Compute micro-average ROC curve and ROC area\n",
    "#     fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
    "#     roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "#     F1 calcualation\n",
    "    # Compute F1 micro\n",
    "    y_true = pd.read_csv(\"/data/home/gicgpu/Know-Your-Intent/datasets/KL/AskUbuntu/test.csv\", header=None, sep='\\t')\n",
    "#     y_true_results = y_true[['ATAD', 'CFC', 'Corporate Tax',\n",
    "#        'Country-by-Country Reporting', 'DAC6', 'Digital Tax',\n",
    "#        'Double Tax Treaty', 'Economic substance', 'GAAR',\n",
    "#        'Goods and services tax (GST)', 'Interest deductibility', 'Local file',\n",
    "#        'MLI', 'Management of a Tax Function', 'Mandatory disclosure rules',\n",
    "#        'Master file', 'Mutual agreement procedure', 'Permanent Establishment',\n",
    "#        'Principal purpose test', 'Section 892', 'Sovereign Immunity',\n",
    "#        'Tax Governance Framework', 'Tax audit', 'Tax compliance',\n",
    "#        'Tax dispute', 'Value-added tax (VAT)', 'Withholding Tax']]\n",
    "    \n",
    "    y_true_results = y_true[1]\n",
    "    pred_results = pd.DataFrame(all_logits, columns=label_list)\n",
    "    pred_results = pred_results.idxmax(axis=1)\n",
    "#     print(y_true_results)\n",
    "    f1_micro = f1_score(y_true_results, pred_results, average='micro')\n",
    "\n",
    "    \n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "#               'loss': tr_loss/nb_tr_steps,\n",
    "              'roc_auc': roc_auc,\n",
    "             'weighted f1_score': f1_micro}\n",
    "    \n",
    "    output_eval_file = os.path.join(args['output_dir'], \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#             writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, args['max_seq_length'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this in train_features:\n",
    "#     print(this.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:38:05 - INFO - root -   ***** Running training *****\n",
      "04/24/2019 11:38:05 - INFO - root -     Num examples = 53\n",
      "04/24/2019 11:38:05 - INFO - root -     Batch size = 8\n",
      "04/24/2019 11:38:05 - INFO - root -     Num steps = 132\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in train_features], dtype=torch.float)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "if args['local_rank'] == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args['train_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epocs=args['num_train_epochs']):\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i_ in tqdm(range(int(num_epocs)), desc=\"Epoch\"):\n",
    "\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "    #             scheduler.batch_step()\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = args['learning_rate'] * warmup_linear(global_step/t_total, args['warmup_proportion'])\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        logger.info('Loss after epoc {}'.format(tr_loss / nb_tr_steps))\n",
    "        logger.info('Eval after epoc {}'.format(i_+1))\n",
    "        eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_early_stopping(num_epocs=args['num_train_epochs'], patience=4):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "#     train_losses = []\n",
    "#     valid_losses = []\n",
    "#     avg_train_losses = []\n",
    "#     avg_valid_losses = [] \n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i_ in tqdm(range(int(num_epocs)), desc=\"Epoch\"):\n",
    "\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "    #             scheduler.batch_step()\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = args['learning_rate'] * warmup_linear(global_step/t_total, args['warmup_proportion'])\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        logger.info('Loss after epoc {}'.format(tr_loss / nb_tr_steps))\n",
    "        logger.info('Eval after epoc {}'.format(i_+1))\n",
    "        eval_results = eval()\n",
    "        # instead of early stopping on val loss we want to stop if f1 stops improving\n",
    "        val_f1 = eval_results['weighted f1_score']\n",
    "        early_stopping(val_f1, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Stopping early\")\n",
    "            break\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAUSE Here, Don't train if loading pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BERT layers for 1 epoch\n",
    "model.freeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687141dfde0e43cea2c3071e5170bd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a3f1c9fff44de995560d264331bd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:54:56 - INFO - root -   Loss after epoc 0.08920026783432279\n",
      "04/24/2019 11:54:56 - INFO - root -   Eval after epoc 1\n",
      "04/24/2019 11:54:56 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:54:56 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:54:56 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:00 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:00 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:00 - INFO - root -     eval_loss = 0.391350830239909\n",
      "04/24/2019 11:55:00 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:00 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "Validation loss decreased (inf --> 0.908257).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "fit_with_early_stopping(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91fa7d943f149aab73e1c38171849ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6208d07429b44fbcbdb0592e21d76890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:16 - INFO - root -   Loss after epoc 0.07830638651336942\n",
      "04/24/2019 11:55:16 - INFO - root -   Eval after epoc 1\n",
      "04/24/2019 11:55:16 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:55:16 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:55:16 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:20 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:20 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:20 - INFO - root -     eval_loss = 0.3975697412554707\n",
      "04/24/2019 11:55:20 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:20 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f55515de984fd3aacf31027c30159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:23 - INFO - root -   Loss after epoc 0.06230604116405759\n",
      "04/24/2019 11:55:23 - INFO - root -   Eval after epoc 2\n",
      "04/24/2019 11:55:23 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:55:23 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:55:23 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:28 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:28 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:28 - INFO - root -     eval_loss = 0.4091342211301838\n",
      "04/24/2019 11:55:28 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:28 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c244f703afcb4db5882d228ec8153f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:31 - INFO - root -   Loss after epoc 0.042056096983807426\n",
      "04/24/2019 11:55:31 - INFO - root -   Eval after epoc 3\n",
      "04/24/2019 11:55:31 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:55:31 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:55:31 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:35 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:35 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:35 - INFO - root -     eval_loss = 0.410017046545233\n",
      "04/24/2019 11:55:35 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:35 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f4db77cef64bfdb4196b79b46a0f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:39 - INFO - root -   Loss after epoc 0.035146991589239666\n",
      "04/24/2019 11:55:39 - INFO - root -   Eval after epoc 4\n",
      "04/24/2019 11:55:39 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:55:39 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:55:39 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:43 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:43 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:43 - INFO - root -     eval_loss = 0.4353801820959364\n",
      "04/24/2019 11:55:43 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:43 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bac75eef644e62bdb441a3d0d5ef36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:46 - INFO - root -   Loss after epoc 0.031233990298850194\n",
      "04/24/2019 11:55:46 - INFO - root -   Eval after epoc 5\n",
      "04/24/2019 11:55:46 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:55:46 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:55:46 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:51 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:51 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:51 - INFO - root -     eval_loss = 0.4415225063317588\n",
      "04/24/2019 11:55:51 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:51 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b058b178106d44b6989cd016eb001be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:54 - INFO - root -   Loss after epoc 0.024836386953081404\n",
      "04/24/2019 11:55:54 - INFO - root -   Eval after epoc 6\n",
      "04/24/2019 11:55:54 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:55:54 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:55:54 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:55:58 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:55:58 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:55:58 - INFO - root -     eval_loss = 0.43736291443929076\n",
      "04/24/2019 11:55:58 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:55:58 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7d734e4284483b8af74a0055cfb662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:02 - INFO - root -   Loss after epoc 0.021888407612485544\n",
      "04/24/2019 11:56:02 - INFO - root -   Eval after epoc 7\n",
      "04/24/2019 11:56:02 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:02 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:02 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:06 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:06 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:06 - INFO - root -     eval_loss = 0.4347226123458573\n",
      "04/24/2019 11:56:06 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:06 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3d25f7dd254c69a3c79fb5f9a02782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:09 - INFO - root -   Loss after epoc 0.021972012306962694\n",
      "04/24/2019 11:56:09 - INFO - root -   Eval after epoc 8\n",
      "04/24/2019 11:56:09 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:09 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:09 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:14 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:14 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:14 - INFO - root -     eval_loss = 0.4331461189181677\n",
      "04/24/2019 11:56:14 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:14 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46412d1f2f44c198a044e84b21adc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:17 - INFO - root -   Loss after epoc 0.02228574880531856\n",
      "04/24/2019 11:56:17 - INFO - root -   Eval after epoc 9\n",
      "04/24/2019 11:56:17 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:17 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:17 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:21 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:21 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:21 - INFO - root -     eval_loss = 0.4325304233602115\n",
      "04/24/2019 11:56:21 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:21 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b798bdeda04be9969b24f7be6fb171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:25 - INFO - root -   Loss after epoc 0.018942412255065783\n",
      "04/24/2019 11:56:25 - INFO - root -   Eval after epoc 10\n",
      "04/24/2019 11:56:25 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:25 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:25 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:29 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:29 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:29 - INFO - root -     eval_loss = 0.4317716649467392\n",
      "04/24/2019 11:56:29 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:29 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a705a0e6d48a7b678c58b4d6342ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:33 - INFO - root -   Loss after epoc 0.018687098420092037\n",
      "04/24/2019 11:56:33 - INFO - root -   Eval after epoc 11\n",
      "04/24/2019 11:56:33 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:33 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:33 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:37 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:37 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:37 - INFO - root -     eval_loss = 0.4312311570559229\n",
      "04/24/2019 11:56:37 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:37 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca9238bb0b94624ad307e4a41dfaede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:40 - INFO - root -   Loss after epoc 0.01876332251621144\n",
      "04/24/2019 11:56:40 - INFO - root -   Eval after epoc 12\n",
      "04/24/2019 11:56:40 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:40 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:40 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:44 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:44 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:44 - INFO - root -     eval_loss = 0.4307073441866253\n",
      "04/24/2019 11:56:44 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:44 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f851eabb9444822a4eb2578ac9bc477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:48 - INFO - root -   Loss after epoc 0.020180503704718182\n",
      "04/24/2019 11:56:48 - INFO - root -   Eval after epoc 13\n",
      "04/24/2019 11:56:48 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:48 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:48 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:52 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:56:52 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:56:52 - INFO - root -     eval_loss = 0.43013481835701634\n",
      "04/24/2019 11:56:52 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:56:52 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524cb6ef266e45349fa5ddea0bee6761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:56:56 - INFO - root -   Loss after epoc 0.019443800672888756\n",
      "04/24/2019 11:56:56 - INFO - root -   Eval after epoc 14\n",
      "04/24/2019 11:56:56 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:56:56 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:56:56 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:57:00 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:57:00 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:57:00 - INFO - root -     eval_loss = 0.4295495750515589\n",
      "04/24/2019 11:57:00 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:57:00 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0794c4febf4ba2b3bc714e45dca4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:57:03 - INFO - root -   Loss after epoc 0.021420210599899292\n",
      "04/24/2019 11:57:03 - INFO - root -   Eval after epoc 15\n",
      "04/24/2019 11:57:03 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:57:03 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:57:03 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:57:08 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:57:08 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:57:08 - INFO - root -     eval_loss = 0.42897934592994197\n",
      "04/24/2019 11:57:08 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:57:08 - INFO - root -     weighted f1_score = 0.908256880733945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6ce62f79f14616962c956b44975c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:57:11 - INFO - root -   Loss after epoc 0.020254544647676603\n",
      "04/24/2019 11:57:11 - INFO - root -   Eval after epoc 16\n",
      "04/24/2019 11:57:11 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:57:11 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:57:11 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-457-ba8cb2bcd5c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-427-9e5cfcc01b41>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(num_epocs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss after epoc {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnb_tr_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eval after epoc {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-450-cd791d473d21>\u001b[0m in \u001b[0;36meval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtmp_eval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#         logits = logits.detach().cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-325-59d65140c3b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    711\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    712\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-pretrain/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:51:35 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:51:35 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:51:35 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:51:39 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:51:39 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:51:39 - INFO - root -     eval_loss = 1.5185668979372298\n",
      "04/24/2019 11:51:39 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:51:39 - INFO - root -     weighted f1_score = 0.3394495412844037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5185668979372298,\n",
       " 'eval_accuracy': tensor(0),\n",
       " 'roc_auc': {},\n",
       " 'weighted f1_score': 0.3394495412844037}"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_bert_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58de3f44dccd40af89fc172d3e5cf48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e016236c1d9e494484e8ce6ff4fd0d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:51:46 - INFO - root -   Loss after epoc 1.5162230219159807\n",
      "04/24/2019 11:51:46 - INFO - root -   Eval after epoc 1\n",
      "04/24/2019 11:51:46 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:51:46 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:51:46 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 0, 1], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:51:50 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:51:50 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:51:50 - INFO - root -     eval_loss = 1.4198031084878104\n",
      "04/24/2019 11:51:50 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:51:50 - INFO - root -     weighted f1_score = 0.5596330275229358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "Validation loss decreased (inf --> 0.559633).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0175afdb9974dd4886d788df8575602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:51:56 - INFO - root -   Loss after epoc 1.329736522265843\n",
      "04/24/2019 11:51:56 - INFO - root -   Eval after epoc 2\n",
      "04/24/2019 11:51:56 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:51:56 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:51:56 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:01 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:52:01 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:52:01 - INFO - root -     eval_loss = 1.104286892073495\n",
      "04/24/2019 11:52:01 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:52:01 - INFO - root -     weighted f1_score = 0.8256880733944955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23139176667c47ae8b897435b7bfa7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:04 - INFO - root -   Loss after epoc 0.8264405684811729\n",
      "04/24/2019 11:52:04 - INFO - root -   Eval after epoc 3\n",
      "04/24/2019 11:52:04 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:52:04 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:52:04 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([0, 1, 0, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:08 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:52:08 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:52:08 - INFO - root -     eval_loss = 0.6545589310782296\n",
      "04/24/2019 11:52:08 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:52:08 - INFO - root -     weighted f1_score = 0.8348623853211009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "EarlyStopping counter: 2 out of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacb4e304a3e43d08843ebc64005f228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:12 - INFO - root -   Loss after epoc 0.30761673620768953\n",
      "04/24/2019 11:52:12 - INFO - root -   Eval after epoc 4\n",
      "04/24/2019 11:52:12 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:52:12 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:52:12 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:16 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:52:16 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:52:16 - INFO - root -     eval_loss = 0.4657659722226007\n",
      "04/24/2019 11:52:16 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:52:16 - INFO - root -     weighted f1_score = 0.8899082568807339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "EarlyStopping counter: 3 out of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc0c8483daa4e7aaf21eb0a518f8d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=7, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:19 - INFO - root -   Loss after epoc 0.13255969594631875\n",
      "04/24/2019 11:52:19 - INFO - root -   Eval after epoc 5\n",
      "04/24/2019 11:52:19 - INFO - root -   ***** Running evaluation *****\n",
      "04/24/2019 11:52:19 - INFO - root -     Num examples = 109\n",
      "04/24/2019 11:52:19 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2019 11:52:24 - INFO - root -   ***** Eval results *****\n",
      "04/24/2019 11:52:24 - INFO - root -     eval_accuracy = tensor(0)\n",
      "04/24/2019 11:52:24 - INFO - root -     eval_loss = 0.3901435684944902\n",
      "04/24/2019 11:52:24 - INFO - root -     roc_auc = {}\n",
      "04/24/2019 11:52:24 - INFO - root -     weighted f1_score = 0.8990825688073395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "fit_with_early_stopping(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on train set first, now train on val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args['do_train']:\n",
    "    train_examples = processor.get_dev_examples(args['full_data_dir'], size=args['train_size'])\n",
    "#     train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, args['max_seq_length'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:48:15 - INFO - root -   ***** Running training *****\n",
      "04/16/2019 03:48:15 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:48:15 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:48:15 - INFO - root -     Num steps = 495\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in train_features], dtype=torch.float)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "if args['local_rank'] == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args['train_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547bf45fdc1c419a87a53aab1a681cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75f70ab95c74aac974e56fffd3022de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:48:46 - INFO - root -   Loss after epoc 3.405038070678711\n",
      "04/16/2019 03:48:46 - INFO - root -   Eval after epoc 1\n",
      "04/16/2019 03:48:56 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:48:56 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:48:56 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:49:03 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:49:03 - INFO - root -     eval_accuracy = 0.9139543615206324\n",
      "04/16/2019 03:49:03 - INFO - root -     eval_loss = 3.4611298382282256\n",
      "04/16/2019 03:49:03 - INFO - root -     roc_auc = {0: 0.6948391922213912, 1: 0.7912371134020618, 2: 0.531578947368421, 3: 0.6554659498207884, 4: 0.7497794766245223, 5: 0.799342105263158, 6: 0.722872340425532, 7: 0.993923611111111, 8: 0.4988780852655198, 9: 0.8033154121863799, 10: 0.9086021505376344, 11: 0.6963350785340314, 12: 0.9148936170212767, 13: 0.818342151675485, 14: 0.9207446808510638, 15: 0.7818100358422938, 16: 0.5489902767389678, 17: 0.758894442810938, 18: 0.9316239316239315, 19: 0.9968911917098445, 20: 0.9670781893004115, 21: 0.8257168458781362, 22: 0.7328947368421053, 23: 0.8949652777777778, 24: 0.7067395264116576, 25: 0.5841942148760331, 26: 0.7894965277777778, 'micro': 0.7653860293435031}\n",
      "04/16/2019 03:49:03 - INFO - root -     weighted f1_score = 0.3612500422786286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.361250).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e6b4bb868944f5ab04ac02584f8571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:49:17 - INFO - root -   Loss after epoc 3.393074116706848\n",
      "04/16/2019 03:49:17 - INFO - root -   Eval after epoc 2\n",
      "04/16/2019 03:49:26 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:49:26 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:49:26 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:49:34 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:49:34 - INFO - root -     eval_accuracy = 0.9070333013630877\n",
      "04/16/2019 03:49:34 - INFO - root -     eval_loss = 3.179743395447731\n",
      "04/16/2019 03:49:34 - INFO - root -     roc_auc = {0: 0.700074794315632, 1: 0.7809278350515464, 2: 0.5375, 3: 0.6586021505376344, 4: 0.7547780064686856, 5: 0.8157894736842104, 6: 0.7308510638297873, 7: 0.9956597222222222, 8: 0.5026178010471204, 9: 0.8051075268817205, 10: 0.9130824372759856, 11: 0.7053103964098728, 12: 0.9297872340425533, 13: 0.823045267489712, 14: 0.9361702127659575, 15: 0.7889784946236559, 16: 0.5646970830216903, 17: 0.7630108791531902, 18: 0.9452991452991453, 19: 1.0, 20: 0.9735449735449735, 21: 0.8337813620071685, 22: 0.7572368421052631, 23: 0.9036458333333333, 24: 0.7289617486338797, 25: 0.5973657024793388, 26: 0.8111979166666667, 'micro': 0.7746703404841567}\n",
      "04/16/2019 03:49:34 - INFO - root -     weighted f1_score = 0.3809517157788392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6d43cfd2d24f7b8251a1495f385450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:49:46 - INFO - root -   Loss after epoc 3.03188125371933\n",
      "04/16/2019 03:49:46 - INFO - root -   Eval after epoc 3\n",
      "04/16/2019 03:49:55 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:49:55 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:49:55 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:50:03 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:50:03 - INFO - root -     eval_accuracy = 0.8963711237666583\n",
      "04/16/2019 03:50:03 - INFO - root -     eval_loss = 2.758124282360077\n",
      "04/16/2019 03:50:03 - INFO - root -     roc_auc = {0: 0.7105459985041137, 1: 0.779639175257732, 2: 0.5546052631578947, 3: 0.6698028673835126, 4: 0.7591884739782417, 5: 0.8282894736842106, 6: 0.7457446808510638, 7: 0.9965277777777778, 8: 0.5071054599850411, 9: 0.8095878136200717, 10: 0.9238351254480287, 11: 0.7083021690351533, 12: 0.9569148936170213, 13: 0.8271604938271605, 14: 0.9627659574468085, 15: 0.7979390681003584, 16: 0.5863874345549739, 17: 0.7738900323434283, 18: 0.9521367521367521, 19: 1.0, 20: 0.9776601998824221, 21: 0.8413978494623655, 22: 0.8026315789473684, 23: 0.9131944444444445, 24: 0.7613843351548271, 25: 0.6252582644628099, 26: 0.8285590277777778, 'micro': 0.7904204713824718}\n",
      "04/16/2019 03:50:03 - INFO - root -     weighted f1_score = 0.3976903032124143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1550282708f74fa1b860fd3e5637713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:50:15 - INFO - root -   Loss after epoc 2.623803045749664\n",
      "04/16/2019 03:50:15 - INFO - root -   Eval after epoc 4\n",
      "04/16/2019 03:50:24 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:50:24 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:50:24 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:50:32 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:50:32 - INFO - root -     eval_accuracy = 0.8782267161089965\n",
      "04/16/2019 03:50:32 - INFO - root -     eval_loss = 2.282428666353226\n",
      "04/16/2019 03:50:32 - INFO - root -     roc_auc = {0: 0.7210172026925953, 1: 0.7719072164948454, 2: 0.6328947368421053, 3: 0.689516129032258, 4: 0.7756542193472508, 5: 0.8651315789473684, 6: 0.7542553191489362, 7: 0.9982638888888888, 8: 0.5534779356768885, 9: 0.8131720430107526, 10: 0.9278673835125448, 11: 0.7217651458489155, 12: 0.9659574468085106, 13: 0.8330393885949442, 14: 0.972872340425532, 15: 0.818100358422939, 16: 0.6387434554973822, 17: 0.7874154660394, 18: 0.9555555555555555, 19: 1.0, 20: 0.9882422104644326, 21: 0.8508064516129031, 22: 0.831578947368421, 23: 0.920138888888889, 24: 0.7839708561020037, 25: 0.6590909090909091, 26: 0.8485243055555556, 'micro': 0.8103192433520368}\n",
      "04/16/2019 03:50:32 - INFO - root -     weighted f1_score = 0.39940196664481686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fd5a57ebec4ebb99f8acee04753514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:50:45 - INFO - root -   Loss after epoc 2.1838810455799105\n",
      "04/16/2019 03:50:45 - INFO - root -   Eval after epoc 5\n",
      "04/16/2019 03:50:54 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:50:54 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:50:54 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:51:01 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:51:01 - INFO - root -     eval_accuracy = 0.8576505810323388\n",
      "04/16/2019 03:51:01 - INFO - root -     eval_loss = 1.8611076545715333\n",
      "04/16/2019 03:51:01 - INFO - root -     roc_auc = {0: 0.7329842931937173, 1: 0.7603092783505155, 2: 0.7118421052631578, 3: 0.7155017921146953, 4: 0.804175242575713, 5: 0.9072368421052631, 6: 0.7718085106382978, 7: 1.0, 8: 0.6110695587135377, 9: 0.8118279569892473, 10: 0.935483870967742, 11: 0.7292445774121167, 12: 0.9797872340425533, 13: 0.8430335097001764, 14: 0.9845744680851064, 15: 0.8561827956989249, 16: 0.6985789080029917, 17: 0.8021170244045869, 18: 0.9555555555555555, 19: 1.0, 20: 0.9911816578483246, 21: 0.8575268817204301, 22: 0.8513157894736842, 23: 0.9314236111111112, 24: 0.8163934426229508, 25: 0.7050619834710744, 26: 0.85546875, 'micro': 0.8326896725871078}\n",
      "04/16/2019 03:51:01 - INFO - root -     weighted f1_score = 0.41557743532342273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 4\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "fit_with_early_stopping(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd658797a5d34d67aeb2d1e8a74f3a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce91f395ab61416abe7b804cd6d74e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:52:35 - INFO - root -   Loss after epoc 1.8854332876205444\n",
      "04/16/2019 03:52:35 - INFO - root -   Eval after epoc 1\n",
      "04/16/2019 03:52:44 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:52:44 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:52:44 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:52:52 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:52:52 - INFO - root -     eval_accuracy = 0.8557800187004937\n",
      "04/16/2019 03:52:52 - INFO - root -     eval_loss = 1.8312656980752946\n",
      "04/16/2019 03:52:52 - INFO - root -     roc_auc = {0: 0.7359760658189978, 1: 0.7603092783505155, 2: 0.7171052631578947, 3: 0.7172939068100358, 4: 0.805351367244928, 5: 0.9072368421052631, 6: 0.7712765957446809, 7: 1.0, 8: 0.6155572176514585, 9: 0.8122759856630825, 10: 0.9345878136200717, 11: 0.7314884068810771, 12: 0.9803191489361702, 13: 0.8447971781305115, 14: 0.9851063829787234, 15: 0.8615591397849462, 16: 0.706058339566193, 17: 0.8027050867391944, 18: 0.9555555555555555, 19: 1.0, 20: 0.9917695473251029, 21: 0.8579749103942652, 22: 0.8532894736842106, 23: 0.9331597222222223, 24: 0.8174863387978142, 25: 0.7079028925619836, 26: 0.8563368055555556, 'micro': 0.8343116976675965}\n",
      "04/16/2019 03:52:52 - INFO - root -     weighted f1_score = 0.4182351075638143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0708a02bb4c42babb51a8fd77b303a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:53:04 - INFO - root -   Loss after epoc 1.8100076556205749\n",
      "04/16/2019 03:53:04 - INFO - root -   Eval after epoc 2\n",
      "04/16/2019 03:53:14 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:53:14 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:53:14 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:53:21 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:53:21 - INFO - root -     eval_accuracy = 0.8490460256133416\n",
      "04/16/2019 03:53:21 - INFO - root -     eval_loss = 1.7343215250968933\n",
      "04/16/2019 03:53:21 - INFO - root -     roc_auc = {0: 0.7389678384442783, 1: 0.7590206185567011, 2: 0.7480263157894737, 3: 0.7262544802867383, 4: 0.8097618347544838, 5: 0.9098684210526315, 6: 0.773404255319149, 7: 1.0, 8: 0.6327599102468212, 9: 0.8136200716845878, 10: 0.9368279569892474, 11: 0.7344801795063575, 12: 0.9797872340425533, 13: 0.8506760728982952, 14: 0.9851063829787234, 15: 0.8705197132616487, 16: 0.7329842931937173, 17: 0.8065274919141429, 18: 0.9555555555555555, 19: 1.0, 20: 0.9929453262786596, 21: 0.8615591397849461, 22: 0.8605263157894737, 23: 0.9366319444444445, 24: 0.8262295081967213, 25: 0.7223657024793388, 26: 0.8563368055555556, 'micro': 0.8407730613123995}\n",
      "04/16/2019 03:53:21 - INFO - root -     weighted f1_score = 0.4131896578122503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9468526a8ad547168f28937f9ed6410a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:53:34 - INFO - root -   Loss after epoc 1.7063525021076202\n",
      "04/16/2019 03:53:34 - INFO - root -   Eval after epoc 3\n",
      "04/16/2019 03:53:43 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:53:43 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:53:43 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:53:50 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:53:50 - INFO - root -     eval_accuracy = 0.8426861233181424\n",
      "04/16/2019 03:53:50 - INFO - root -     eval_loss = 1.5936561942100524\n",
      "04/16/2019 03:53:50 - INFO - root -     roc_auc = {0: 0.737471952131638, 1: 0.7590206185567011, 2: 0.7868421052631579, 3: 0.7401433691756272, 4: 0.8247574242869744, 5: 0.9151315789473684, 6: 0.7819148936170213, 7: 1.0, 8: 0.6604338070306657, 9: 0.8167562724014338, 10: 0.9390681003584229, 11: 0.7464472700074795, 12: 0.9803191489361702, 13: 0.8612580834803057, 14: 0.9851063829787234, 15: 0.882168458781362, 16: 0.7711293941660433, 17: 0.8132902087621288, 18: 0.9572649572649573, 19: 1.0, 20: 0.9958847736625513, 21: 0.8651433691756273, 22: 0.8697368421052631, 23: 0.9418402777777778, 24: 0.8375227686703097, 25: 0.7435433884297521, 26: 0.8572048611111112, 'micro': 0.851805654064532}\n",
      "04/16/2019 03:53:50 - INFO - root -     weighted f1_score = 0.4114942027353994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e28053232334add8d4c4d82db3a73e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:54:03 - INFO - root -   Loss after epoc 1.5536206257343292\n",
      "04/16/2019 03:54:03 - INFO - root -   Eval after epoc 4\n",
      "04/16/2019 03:54:12 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:54:12 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:54:12 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:54:20 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:54:20 - INFO - root -     eval_accuracy = 0.837448568055124\n",
      "04/16/2019 03:54:20 - INFO - root -     eval_loss = 1.432760897874832\n",
      "04/16/2019 03:54:20 - INFO - root -     roc_auc = {0: 0.7419596110695588, 1: 0.7564432989690721, 2: 0.819078947368421, 3: 0.7656810035842294, 4: 0.8391649514848574, 5: 0.9223684210526315, 6: 0.7877659574468086, 7: 1.0, 8: 0.6731488406881078, 9: 0.8216845878136201, 10: 0.9439964157706093, 11: 0.7636499626028421, 12: 0.9808510638297873, 13: 0.884185773074662, 14: 0.9845744680851064, 15: 0.8938172043010753, 16: 0.8167539267015707, 17: 0.8191708321082034, 18: 0.9606837606837606, 19: 1.0, 20: 0.9988242210464433, 21: 0.8709677419354839, 22: 0.8835526315789475, 23: 0.9479166666666667, 24: 0.8539162112932605, 25: 0.784349173553719, 26: 0.8572048611111112, 'micro': 0.8660956653162168}\n",
      "04/16/2019 03:54:20 - INFO - root -     weighted f1_score = 0.4234843778586667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef47099363c24f119de1cbd8b21cad73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=25, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:54:32 - INFO - root -   Loss after epoc 1.3964218974113465\n",
      "04/16/2019 03:54:32 - INFO - root -   Eval after epoc 5\n",
      "04/16/2019 03:54:41 - INFO - root -   ***** Running evaluation *****\n",
      "04/16/2019 03:54:41 - INFO - root -     Num examples = 198\n",
      "04/16/2019 03:54:41 - INFO - root -     Batch size = 8\n",
      "04/16/2019 03:54:49 - INFO - root -   ***** Eval results *****\n",
      "04/16/2019 03:54:49 - INFO - root -     eval_accuracy = 0.8413767477478644\n",
      "04/16/2019 03:54:49 - INFO - root -     eval_loss = 1.2948837733268739\n",
      "04/16/2019 03:54:49 - INFO - root -     roc_auc = {0: 0.7471952131637996, 1: 0.7564432989690723, 2: 0.8368421052631579, 3: 0.7948028673835125, 4: 0.8573948838576889, 5: 0.9322368421052631, 6: 0.800531914893617, 7: 1.0, 8: 0.6970830216903515, 9: 0.8293010752688172, 10: 0.9493727598566308, 11: 0.7733732236350037, 12: 0.9819148936170212, 13: 0.9129923574368018, 14: 0.9861702127659575, 15: 0.9005376344086021, 16: 0.855646970830217, 17: 0.8291678917965304, 18: 0.9589743589743589, 19: 1.0, 20: 0.9994121105232217, 21: 0.8812724014336917, 22: 0.8960526315789474, 23: 0.9505208333333334, 24: 0.8735883424408015, 25: 0.8370351239669421, 26: 0.87109375, 'micro': 0.8802460516983361}\n",
      "04/16/2019 03:54:49 - INFO - root -     weighted f1_score = 0.44208462147757643\n"
     ]
    }
   ],
   "source": [
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 03:55:13 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/gicgpu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/16/2019 03:55:13 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/gicgpu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp7zyj80fq\n",
      "04/16/2019 03:55:17 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "# output_model_file = os.path.join(PYTORCH_PRETRAINED_BERT_CACHE, \"bert27catsround0.1.bin\")\n",
    "output_model_file = os.path.join('/data/home/gicgpu/sprint4/model/', \"bert27trainontrainthenVal.bin\")\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file)\n",
    "model = BertForMultiLabelSequenceClassification.from_pretrained(args['bert_model'], num_labels = num_labels, state_dict=model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/home/gicgpu/sprint4/data/test.csv')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(DATA_PATH, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(Path(DATA_PATH, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(\"Index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(Path(DATA_PATH, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.Reference.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, path, test_filename='test.csv'):\n",
    "    predict_processor = MultiLabelTextProcessor(path)\n",
    "    test_examples = predict_processor.get_test_examples(path, test_filename, size=-1)\n",
    "    \n",
    "    # Hold input data for returning it \n",
    "    input_data = [{ 'Reference': input_example.guid, 'text': input_example.text_a } for input_example in test_examples]\n",
    "\n",
    "    test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    \n",
    "    logger.info(\"***** Running prediction *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    all_logits = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(tqdm(test_dataloader, desc=\"Prediction Iteration\")):\n",
    "        input_ids, input_mask, segment_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            logits = logits.sigmoid()\n",
    "\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    return pd.merge(pd.DataFrame(input_data), pd.DataFrame(all_logits, columns=label_list), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2019 03:11:07 - INFO - root -   ***** Running prediction *****\n",
      "04/17/2019 03:11:07 - INFO - root -     Num examples = 3438\n",
      "04/17/2019 03:11:07 - INFO - root -     Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8889ebbe72e43878a4e32ebf666299c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Prediction Iteration', max=430, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = predict(model, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>text</th>\n",
       "      <th>(Management of a) Tax function</th>\n",
       "      <th>Anti-Tax Avoidance Directive (ATAD)</th>\n",
       "      <th>Controlled Foreign Corporation</th>\n",
       "      <th>Corporate Tax</th>\n",
       "      <th>Country-by-Country Reporting</th>\n",
       "      <th>Digital Tax</th>\n",
       "      <th>Double Tax Treaty</th>\n",
       "      <th>EU Mandatory Disclosure Directive (DAC6)</th>\n",
       "      <th>...</th>\n",
       "      <th>Permanent Establishment (PE)</th>\n",
       "      <th>Principal purpose test</th>\n",
       "      <th>Section 892</th>\n",
       "      <th>Sovereign Immunity</th>\n",
       "      <th>Tax Governance Framework</th>\n",
       "      <th>Tax audit</th>\n",
       "      <th>Tax compliance</th>\n",
       "      <th>Tax dispute</th>\n",
       "      <th>Value-added tax (VAT)</th>\n",
       "      <th>Withholding Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>OECD/G20 Base Erosion and Profit Shifting Project</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>Addressing the Tax Challenges of the</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>Digitalisation of the Economy</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034298</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.085376</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Public Consultation Document</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>Public Consultation Document</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>Following a mandate by G20 Finance Ministers i...</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.031505</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.002282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>on BEPS, working through its Task Force on the...</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.957679</td>\n",
       "      <td>0.022958</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034356</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.012617</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.019606</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.004676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>Interim Report in March 2018, Tax Challenges A...</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.001008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>Report 2018. One of the important conclusions ...</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.002067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>review the impact of digitalisation on nexus a...</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.001635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>continue working together towards a final repo...</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>based long-term solution, with an update in 2019.</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>Since the delivery of the Interim Report, the ...</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>work and several proposals emerged that could ...</td>\n",
       "      <td>0.018148</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.064779</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.001048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>broader challenges arising from the digitalisa...</td>\n",
       "      <td>0.087101</td>\n",
       "      <td>0.021356</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.995179</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135562</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.039437</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.209587</td>\n",
       "      <td>0.010247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>BEPS issues. The work on these proposals is be...</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>basis; their examination does not represent a ...</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.002170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>Framework beyond exploring these proposals. In...</td>\n",
       "      <td>0.018927</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.002924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>agreed to hold a public consultation on possib...</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.001134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>from the digitalisation of the economy on 13 a...</td>\n",
       "      <td>0.039751</td>\n",
       "      <td>0.011471</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.976524</td>\n",
       "      <td>0.026930</td>\n",
       "      <td>0.009957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084866</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.193170</td>\n",
       "      <td>0.008214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>Conference Centre in Paris, France. The object...</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>opportunity to provide input early in the proc...</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>As part of this public consultation, this cons...</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.037626</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.002887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>discussed by the Inclusive Framework at a high...</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>public on a number of policy issues and techni...</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15</td>\n",
       "      <td>assist members of the Inclusive Framework in t...</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15</td>\n",
       "      <td>report to the G20 in 2020.</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15</td>\n",
       "      <td>Interested parties are invited to send their c...</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15</td>\n",
       "      <td>Comments should be sent by 1 March 2019 at the...</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.003224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>Word format (in order to facilitate their dist...</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.001308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>Another instance GAAR has been invoked relates...</td>\n",
       "      <td>0.049858</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.012544</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.021042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>Hitesh Gajaria, head of tax, KPMG India, belie...</td>\n",
       "      <td>0.036279</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.019528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>If companies are able to demonstrate commercia...</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.016512</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.008930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>Often, deals are structured in certain ways to...</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.001010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>But if it strikes a deal with a single buyer f...</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.010075</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.016076</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>There are apprehensions that given the sweepin...</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.009952</td>\n",
       "      <td>0.013719</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.010124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>With the advent of GAAR, the number of litiga...</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.011993</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.014087</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.009705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Chinese sales tax: bye-bye business tax</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.030789</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.093785</td>\n",
       "      <td>0.003743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>On March 5, 2016 the long awaited last stage o...</td>\n",
       "      <td>0.082455</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.028040</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.095444</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.008030</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.302574</td>\n",
       "      <td>0.003626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The business tax which had been applicable for...</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.634783</td>\n",
       "      <td>0.031122</td>\n",
       "      <td>0.068106</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.011462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.038627</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.102267</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.377078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>In particular at an international level the Ch...</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.613025</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.003647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>According to the previously submitted draft la...</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The 4 remaining sectors where business tax is ...</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.076567</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>0.009882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>For tax assessments of the cross-border servic...</td>\n",
       "      <td>0.075590</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.030423</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>0.035146</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.064351</td>\n",
       "      <td>0.205310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Should taxpayers with foreign loans have to wi...</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.019188</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.075672</td>\n",
       "      <td>0.010211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Can the input tax for necessary accommodation ...</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>If current office rental contracts expire in t...</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The inclusion of construction services in the ...</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Regarding current contracts and transaction wh...</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.000969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>For current transactions not yet summarised by...</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The current business model and the type of tra...</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>As the finalisation of the sales tax reform ca...</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>Opinion | Tech changing the face of indirect t...</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>Tax as a subject has undoubtedly gained priori...</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.083787</td>\n",
       "      <td>0.056272</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.054327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033279</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>0.145497</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>0.075449</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.232692</td>\n",
       "      <td>0.008269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>This is primarily on account of the pace at wh...</td>\n",
       "      <td>0.793611</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.035558</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.088239</td>\n",
       "      <td>0.534807</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.032928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.084340</td>\n",
       "      <td>0.150643</td>\n",
       "      <td>0.063706</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.922894</td>\n",
       "      <td>0.108795</td>\n",
       "      <td>0.328709</td>\n",
       "      <td>0.019598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>With governments gearing up with technology an...</td>\n",
       "      <td>0.341052</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.021498</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.018346</td>\n",
       "      <td>0.021146</td>\n",
       "      <td>0.045643</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.998094</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.014905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>There are many examples of companies deploying...</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>However, organizations can no longer stop at j...</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.118482</td>\n",
       "      <td>0.078150</td>\n",
       "      <td>0.108975</td>\n",
       "      <td>0.293462</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>0.182493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053615</td>\n",
       "      <td>0.013594</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>0.119367</td>\n",
       "      <td>0.438786</td>\n",
       "      <td>0.136498</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.018919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>Organizations are not shy about embarking upon...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.093839</td>\n",
       "      <td>0.077183</td>\n",
       "      <td>0.070110</td>\n",
       "      <td>0.203103</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.106270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>0.055816</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.314895</td>\n",
       "      <td>0.383018</td>\n",
       "      <td>0.103285</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>0.686949</td>\n",
       "      <td>0.020997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>As in every field, evolution and innovation wi...</td>\n",
       "      <td>0.651038</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.251662</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.019672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.041824</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.056590</td>\n",
       "      <td>0.003524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3438 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "0                                                    15   \n",
       "1                                                    15   \n",
       "2                                                    15   \n",
       "3                                                    15   \n",
       "4                                                    15   \n",
       "5                                                    15   \n",
       "6                                                    15   \n",
       "7                                                    15   \n",
       "8                                                    15   \n",
       "9                                                    15   \n",
       "10                                                   15   \n",
       "11                                                   15   \n",
       "12                                                   15   \n",
       "13                                                   15   \n",
       "14                                                   15   \n",
       "15                                                   15   \n",
       "16                                                   15   \n",
       "17                                                   15   \n",
       "18                                                   15   \n",
       "19                                                   15   \n",
       "20                                                   15   \n",
       "21                                                   15   \n",
       "22                                                   15   \n",
       "23                                                   15   \n",
       "24                                                   15   \n",
       "25                                                   15   \n",
       "26                                                   15   \n",
       "27                                                   15   \n",
       "28                                                   15   \n",
       "29                                                   15   \n",
       "...                                                 ...   \n",
       "3408  https://economictimes.indiatimes.com/news/econ...   \n",
       "3409  https://economictimes.indiatimes.com/news/econ...   \n",
       "3410  https://economictimes.indiatimes.com/news/econ...   \n",
       "3411  https://economictimes.indiatimes.com/news/econ...   \n",
       "3412  https://economictimes.indiatimes.com/news/econ...   \n",
       "3413  https://economictimes.indiatimes.com/news/econ...   \n",
       "3414  https://economictimes.indiatimes.com/news/econ...   \n",
       "3415  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3416  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3417  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3418  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3419  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3420  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3421  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3422  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3423  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3424  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3425  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3426  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3427  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3428  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3429  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3430  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3431  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3432  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3433  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3434  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3435  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3436  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3437  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     OECD/G20 Base Erosion and Profit Shifting Project   \n",
       "1                  Addressing the Tax Challenges of the   \n",
       "2                         Digitalisation of the Economy   \n",
       "3                          Public Consultation Document   \n",
       "4                          Public Consultation Document   \n",
       "5     Following a mandate by G20 Finance Ministers i...   \n",
       "6     on BEPS, working through its Task Force on the...   \n",
       "7     Interim Report in March 2018, Tax Challenges A...   \n",
       "8     Report 2018. One of the important conclusions ...   \n",
       "9     review the impact of digitalisation on nexus a...   \n",
       "10    continue working together towards a final repo...   \n",
       "11    based long-term solution, with an update in 2019.   \n",
       "12    Since the delivery of the Interim Report, the ...   \n",
       "13    work and several proposals emerged that could ...   \n",
       "14    broader challenges arising from the digitalisa...   \n",
       "15    BEPS issues. The work on these proposals is be...   \n",
       "16    basis; their examination does not represent a ...   \n",
       "17    Framework beyond exploring these proposals. In...   \n",
       "18    agreed to hold a public consultation on possib...   \n",
       "19    from the digitalisation of the economy on 13 a...   \n",
       "20    Conference Centre in Paris, France. The object...   \n",
       "21    opportunity to provide input early in the proc...   \n",
       "22    As part of this public consultation, this cons...   \n",
       "23    discussed by the Inclusive Framework at a high...   \n",
       "24    public on a number of policy issues and techni...   \n",
       "25    assist members of the Inclusive Framework in t...   \n",
       "26                           report to the G20 in 2020.   \n",
       "27    Interested parties are invited to send their c...   \n",
       "28    Comments should be sent by 1 March 2019 at the...   \n",
       "29    Word format (in order to facilitate their dist...   \n",
       "...                                                 ...   \n",
       "3408  Another instance GAAR has been invoked relates...   \n",
       "3409  Hitesh Gajaria, head of tax, KPMG India, belie...   \n",
       "3410  If companies are able to demonstrate commercia...   \n",
       "3411  Often, deals are structured in certain ways to...   \n",
       "3412  But if it strikes a deal with a single buyer f...   \n",
       "3413  There are apprehensions that given the sweepin...   \n",
       "3414  With the advent of GAAR, the number of litiga...   \n",
       "3415            Chinese sales tax: bye-bye business tax   \n",
       "3416  On March 5, 2016 the long awaited last stage o...   \n",
       "3417  The business tax which had been applicable for...   \n",
       "3418  In particular at an international level the Ch...   \n",
       "3419  According to the previously submitted draft la...   \n",
       "3420  The 4 remaining sectors where business tax is ...   \n",
       "3421  For tax assessments of the cross-border servic...   \n",
       "3422  Should taxpayers with foreign loans have to wi...   \n",
       "3423  Can the input tax for necessary accommodation ...   \n",
       "3424  If current office rental contracts expire in t...   \n",
       "3425  The inclusion of construction services in the ...   \n",
       "3426  Regarding current contracts and transaction wh...   \n",
       "3427  For current transactions not yet summarised by...   \n",
       "3428  The current business model and the type of tra...   \n",
       "3429  As the finalisation of the sales tax reform ca...   \n",
       "3430  Opinion | Tech changing the face of indirect t...   \n",
       "3431  Tax as a subject has undoubtedly gained priori...   \n",
       "3432  This is primarily on account of the pace at wh...   \n",
       "3433  With governments gearing up with technology an...   \n",
       "3434  There are many examples of companies deploying...   \n",
       "3435  However, organizations can no longer stop at j...   \n",
       "3436  Organizations are not shy about embarking upon...   \n",
       "3437  As in every field, evolution and innovation wi...   \n",
       "\n",
       "      (Management of a) Tax function  Anti-Tax Avoidance Directive (ATAD)  \\\n",
       "0                           0.003044                             0.000463   \n",
       "1                           0.002656                             0.001092   \n",
       "2                           0.053961                             0.004193   \n",
       "3                           0.001372                             0.001472   \n",
       "4                           0.001372                             0.001472   \n",
       "5                           0.008591                             0.005943   \n",
       "6                           0.026746                             0.010515   \n",
       "7                           0.003487                             0.000647   \n",
       "8                           0.002212                             0.000510   \n",
       "9                           0.006703                             0.001405   \n",
       "10                          0.005002                             0.000459   \n",
       "11                          0.001820                             0.000570   \n",
       "12                          0.004728                             0.006959   \n",
       "13                          0.018148                             0.000855   \n",
       "14                          0.087101                             0.021356   \n",
       "15                          0.002669                             0.000750   \n",
       "16                          0.004710                             0.000640   \n",
       "17                          0.018927                             0.007333   \n",
       "18                          0.004123                             0.000623   \n",
       "19                          0.039751                             0.011471   \n",
       "20                          0.005230                             0.000684   \n",
       "21                          0.003124                             0.000491   \n",
       "22                          0.000649                             0.037626   \n",
       "23                          0.001420                             0.000990   \n",
       "24                          0.001682                             0.001752   \n",
       "25                          0.003907                             0.001579   \n",
       "26                          0.001605                             0.000699   \n",
       "27                          0.001021                             0.001048   \n",
       "28                          0.002379                             0.000434   \n",
       "29                          0.002200                             0.000497   \n",
       "...                              ...                                  ...   \n",
       "3408                        0.049858                             0.014727   \n",
       "3409                        0.036279                             0.009910   \n",
       "3410                        0.041070                             0.010453   \n",
       "3411                        0.002463                             0.000439   \n",
       "3412                        0.011852                             0.006900   \n",
       "3413                        0.032057                             0.011183   \n",
       "3414                        0.047442                             0.013554   \n",
       "3415                        0.022064                             0.000744   \n",
       "3416                        0.082455                             0.000954   \n",
       "3417                        0.005168                             0.017071   \n",
       "3418                        0.017695                             0.001063   \n",
       "3419                        0.003515                             0.000310   \n",
       "3420                        0.023776                             0.000311   \n",
       "3421                        0.075590                             0.000403   \n",
       "3422                        0.014696                             0.000402   \n",
       "3423                        0.002632                             0.000505   \n",
       "3424                        0.003814                             0.000584   \n",
       "3425                        0.001403                             0.000872   \n",
       "3426                        0.002045                             0.000529   \n",
       "3427                        0.000963                             0.000786   \n",
       "3428                        0.003064                             0.000482   \n",
       "3429                        0.001708                             0.000816   \n",
       "3430                        0.010861                             0.000474   \n",
       "3431                        0.999273                             0.002811   \n",
       "3432                        0.793611                             0.002164   \n",
       "3433                        0.341052                             0.001394   \n",
       "3434                        0.004200                             0.000469   \n",
       "3435                        0.999654                             0.005228   \n",
       "3436                        0.999639                             0.004059   \n",
       "3437                        0.651038                             0.000806   \n",
       "\n",
       "      Controlled Foreign Corporation  Corporate Tax  \\\n",
       "0                           0.000574       0.001024   \n",
       "1                           0.000343       0.001406   \n",
       "2                           0.004587       0.006153   \n",
       "3                           0.000378       0.001080   \n",
       "4                           0.000378       0.001080   \n",
       "5                           0.001007       0.006257   \n",
       "6                           0.004537       0.005921   \n",
       "7                           0.000546       0.000813   \n",
       "8                           0.000543       0.000837   \n",
       "9                           0.001091       0.001370   \n",
       "10                          0.000604       0.000737   \n",
       "11                          0.000335       0.001018   \n",
       "12                          0.000418       0.005379   \n",
       "13                          0.000575       0.002064   \n",
       "14                          0.009792       0.016447   \n",
       "15                          0.000411       0.001218   \n",
       "16                          0.000505       0.001180   \n",
       "17                          0.002821       0.003611   \n",
       "18                          0.000665       0.000808   \n",
       "19                          0.007631       0.013311   \n",
       "20                          0.000313       0.000523   \n",
       "21                          0.000305       0.000705   \n",
       "22                          0.000511       0.005815   \n",
       "23                          0.000428       0.000895   \n",
       "24                          0.000568       0.001608   \n",
       "25                          0.000399       0.000968   \n",
       "26                          0.000291       0.001000   \n",
       "27                          0.000225       0.000975   \n",
       "28                          0.001118       0.001068   \n",
       "29                          0.000579       0.000736   \n",
       "...                              ...            ...   \n",
       "3408                        0.009352       0.012406   \n",
       "3409                        0.006204       0.010076   \n",
       "3410                        0.005262       0.008626   \n",
       "3411                        0.000336       0.001731   \n",
       "3412                        0.002602       0.002903   \n",
       "3413                        0.004470       0.011087   \n",
       "3414                        0.005299       0.008082   \n",
       "3415                        0.003236       0.014245   \n",
       "3416                        0.007778       0.028040   \n",
       "3417                        0.017372       0.634783   \n",
       "3418                        0.001016       0.613025   \n",
       "3419                        0.000548       0.000893   \n",
       "3420                        0.001335       0.076567   \n",
       "3421                        0.006477       0.003338   \n",
       "3422                        0.003005       0.007271   \n",
       "3423                        0.000346       0.001131   \n",
       "3424                        0.000303       0.001284   \n",
       "3425                        0.000308       0.001768   \n",
       "3426                        0.000308       0.001563   \n",
       "3427                        0.000253       0.001174   \n",
       "3428                        0.000512       0.001149   \n",
       "3429                        0.000391       0.000759   \n",
       "3430                        0.000993       0.001270   \n",
       "3431                        0.038319       0.045543   \n",
       "3432                        0.035558       0.008935   \n",
       "3433                        0.009154       0.012293   \n",
       "3434                        0.001151       0.001222   \n",
       "3435                        0.118482       0.078150   \n",
       "3436                        0.093839       0.077183   \n",
       "3437                        0.028906       0.002955   \n",
       "\n",
       "      Country-by-Country Reporting  Digital Tax  Double Tax Treaty  \\\n",
       "0                         0.000703     0.002957           0.001392   \n",
       "1                         0.000622     0.003077           0.002149   \n",
       "2                         0.001270     0.876471           0.007152   \n",
       "3                         0.000919     0.002173           0.001455   \n",
       "4                         0.000919     0.002173           0.001455   \n",
       "5                         0.001127     0.031505           0.005491   \n",
       "6                         0.004654     0.957679           0.022958   \n",
       "7                         0.000546     0.003020           0.001132   \n",
       "8                         0.002185     0.001370           0.001818   \n",
       "9                         0.000573     0.062245           0.002521   \n",
       "10                        0.003870     0.002098           0.001318   \n",
       "11                        0.001003     0.001688           0.001237   \n",
       "12                        0.016447     0.013062           0.003988   \n",
       "13                        0.001958     0.064779           0.003269   \n",
       "14                        0.004307     0.995179           0.030557   \n",
       "15                        0.000701     0.004477           0.001988   \n",
       "16                        0.001001     0.009804           0.003376   \n",
       "17                        0.000617     0.010325           0.003320   \n",
       "18                        0.000518     0.008545           0.002037   \n",
       "19                        0.002523     0.976524           0.026930   \n",
       "20                        0.002188     0.009711           0.001944   \n",
       "21                        0.000841     0.002449           0.001232   \n",
       "22                        0.006393     0.006750           0.003966   \n",
       "23                        0.001051     0.003640           0.001894   \n",
       "24                        0.000487     0.001628           0.002462   \n",
       "25                        0.004840     0.005626           0.001621   \n",
       "26                        0.002148     0.001496           0.001165   \n",
       "27                        0.001373     0.001317           0.001567   \n",
       "28                        0.001523     0.002307           0.001720   \n",
       "29                        0.000989     0.001981           0.001587   \n",
       "...                            ...          ...                ...   \n",
       "3408                      0.012544     0.010344           0.022188   \n",
       "3409                      0.008426     0.008658           0.017370   \n",
       "3410                      0.012070     0.010353           0.016512   \n",
       "3411                      0.001274     0.001072           0.001251   \n",
       "3412                      0.004254     0.003436           0.010075   \n",
       "3413                      0.011582     0.009952           0.013719   \n",
       "3414                      0.013473     0.011993           0.012327   \n",
       "3415                      0.001261     0.030789           0.002825   \n",
       "3416                      0.003912     0.095444           0.003877   \n",
       "3417                      0.031122     0.068106           0.998519   \n",
       "3418                      0.002337     0.003725           0.002846   \n",
       "3419                      0.000605     0.002521           0.001302   \n",
       "3420                      0.003390     0.007648           0.003354   \n",
       "3421                      0.001719     0.068935           0.030423   \n",
       "3422                      0.001090     0.019188           0.002464   \n",
       "3423                      0.000457     0.001706           0.001224   \n",
       "3424                      0.001154     0.001843           0.001491   \n",
       "3425                      0.001151     0.001788           0.001230   \n",
       "3426                      0.000815     0.001487           0.001349   \n",
       "3427                      0.001023     0.000960           0.001830   \n",
       "3428                      0.000491     0.002972           0.001529   \n",
       "3429                      0.001106     0.002494           0.001077   \n",
       "3430                      0.000770     0.007416           0.001275   \n",
       "3431                      0.083787     0.056272           0.004834   \n",
       "3432                      0.088239     0.534807           0.003444   \n",
       "3433                      0.010605     0.021498           0.006212   \n",
       "3434                      0.004758     0.003949           0.002286   \n",
       "3435                      0.108975     0.293462           0.009792   \n",
       "3436                      0.070110     0.203103           0.004800   \n",
       "3437                      0.000828     0.251662           0.002871   \n",
       "\n",
       "      EU Mandatory Disclosure Directive (DAC6)  ...  \\\n",
       "0                                     0.000712  ...   \n",
       "1                                     0.000899  ...   \n",
       "2                                     0.005327  ...   \n",
       "3                                     0.000690  ...   \n",
       "4                                     0.000690  ...   \n",
       "5                                     0.005138  ...   \n",
       "6                                     0.007104  ...   \n",
       "7                                     0.001162  ...   \n",
       "8                                     0.001059  ...   \n",
       "9                                     0.002040  ...   \n",
       "10                                    0.001572  ...   \n",
       "11                                    0.000597  ...   \n",
       "12                                    0.003830  ...   \n",
       "13                                    0.000917  ...   \n",
       "14                                    0.013811  ...   \n",
       "15                                    0.000876  ...   \n",
       "16                                    0.001266  ...   \n",
       "17                                    0.012314  ...   \n",
       "18                                    0.001569  ...   \n",
       "19                                    0.009957  ...   \n",
       "20                                    0.001119  ...   \n",
       "21                                    0.000711  ...   \n",
       "22                                    0.001518  ...   \n",
       "23                                    0.001040  ...   \n",
       "24                                    0.001972  ...   \n",
       "25                                    0.000812  ...   \n",
       "26                                    0.000707  ...   \n",
       "27                                    0.000524  ...   \n",
       "28                                    0.001626  ...   \n",
       "29                                    0.001094  ...   \n",
       "...                                        ...  ...   \n",
       "3408                                  0.003079  ...   \n",
       "3409                                  0.002080  ...   \n",
       "3410                                  0.002434  ...   \n",
       "3411                                  0.000677  ...   \n",
       "3412                                  0.001693  ...   \n",
       "3413                                  0.001689  ...   \n",
       "3414                                  0.002397  ...   \n",
       "3415                                  0.001899  ...   \n",
       "3416                                  0.005237  ...   \n",
       "3417                                  0.011462  ...   \n",
       "3418                                  0.000482  ...   \n",
       "3419                                  0.000907  ...   \n",
       "3420                                  0.000753  ...   \n",
       "3421                                  0.011133  ...   \n",
       "3422                                  0.001516  ...   \n",
       "3423                                  0.000805  ...   \n",
       "3424                                  0.000773  ...   \n",
       "3425                                  0.000496  ...   \n",
       "3426                                  0.000618  ...   \n",
       "3427                                  0.000579  ...   \n",
       "3428                                  0.000941  ...   \n",
       "3429                                  0.000721  ...   \n",
       "3430                                  0.001667  ...   \n",
       "3431                                  0.054327  ...   \n",
       "3432                                  0.032928  ...   \n",
       "3433                                  0.019391  ...   \n",
       "3434                                  0.002011  ...   \n",
       "3435                                  0.182493  ...   \n",
       "3436                                  0.106270  ...   \n",
       "3437                                  0.019672  ...   \n",
       "\n",
       "      Permanent Establishment (PE)  Principal purpose test  Section 892  \\\n",
       "0                         0.001248                0.000536     0.000602   \n",
       "1                         0.001788                0.000371     0.000414   \n",
       "2                         0.034298                0.003201     0.002622   \n",
       "3                         0.000704                0.000393     0.000497   \n",
       "4                         0.000704                0.000393     0.000497   \n",
       "5                         0.003537                0.000819     0.000840   \n",
       "6                         0.034356                0.008705     0.004044   \n",
       "7                         0.002301                0.000729     0.000503   \n",
       "8                         0.001090                0.000962     0.001185   \n",
       "9                         0.001443                0.000641     0.001172   \n",
       "10                        0.001917                0.001507     0.000678   \n",
       "11                        0.000793                0.000478     0.000477   \n",
       "12                        0.002095                0.000831     0.001497   \n",
       "13                        0.001926                0.000719     0.000578   \n",
       "14                        0.135562                0.008460     0.006442   \n",
       "15                        0.001754                0.000633     0.000425   \n",
       "16                        0.001176                0.000620     0.000887   \n",
       "17                        0.002232                0.001633     0.001928   \n",
       "18                        0.001679                0.000636     0.000757   \n",
       "19                        0.084866                0.007342     0.003518   \n",
       "20                        0.001692                0.000480     0.000495   \n",
       "21                        0.001088                0.000444     0.000486   \n",
       "22                        0.001396                0.000830     0.000781   \n",
       "23                        0.000847                0.000493     0.000651   \n",
       "24                        0.002631                0.000823     0.000380   \n",
       "25                        0.000587                0.000436     0.000860   \n",
       "26                        0.000781                0.000476     0.000556   \n",
       "27                        0.000811                0.000400     0.000437   \n",
       "28                        0.001158                0.000902     0.001387   \n",
       "29                        0.001632                0.000646     0.000585   \n",
       "...                            ...                     ...          ...   \n",
       "3408                      0.005260                0.027094     0.003762   \n",
       "3409                      0.004333                0.018520     0.002583   \n",
       "3410                      0.004392                0.017293     0.002611   \n",
       "3411                      0.000818                0.000562     0.000418   \n",
       "3412                      0.002745                0.016076     0.000914   \n",
       "3413                      0.003614                0.012419     0.002584   \n",
       "3414                      0.003808                0.014087     0.002741   \n",
       "3415                      0.008888                0.002547     0.001741   \n",
       "3416                      0.018097                0.008030     0.005391   \n",
       "3417                      0.106869                0.038627     0.003945   \n",
       "3418                      0.012696                0.001150     0.000945   \n",
       "3419                      0.001074                0.000619     0.000584   \n",
       "3420                      0.010018                0.002217     0.001443   \n",
       "3421                      0.010787                0.003028     0.005475   \n",
       "3422                      0.002384                0.001850     0.002931   \n",
       "3423                      0.001853                0.000488     0.000370   \n",
       "3424                      0.001763                0.000407     0.000553   \n",
       "3425                      0.000906                0.000473     0.000468   \n",
       "3426                      0.001258                0.000455     0.000360   \n",
       "3427                      0.000831                0.000423     0.000341   \n",
       "3428                      0.002087                0.000685     0.000476   \n",
       "3429                      0.001117                0.000657     0.000604   \n",
       "3430                      0.001521                0.000661     0.000975   \n",
       "3431                      0.033279                0.015827     0.024430   \n",
       "3432                      0.012902                0.023008     0.084340   \n",
       "3433                      0.013384                0.002195     0.018346   \n",
       "3434                      0.000441                0.001228     0.003636   \n",
       "3435                      0.053615                0.013594     0.071919   \n",
       "3436                      0.032992                0.025257     0.055816   \n",
       "3437                      0.004141                0.003823     0.009188   \n",
       "\n",
       "      Sovereign Immunity  Tax Governance Framework  Tax audit  Tax compliance  \\\n",
       "0               0.000585                  0.001071   0.001626        0.001138   \n",
       "1               0.000652                  0.001134   0.003130        0.001982   \n",
       "2               0.006448                  0.012398   0.002304        0.003404   \n",
       "3               0.000459                  0.000933   0.001599        0.000968   \n",
       "4               0.000459                  0.000933   0.001599        0.000968   \n",
       "5               0.001499                  0.011445   0.002681        0.002059   \n",
       "6               0.006191                  0.012617   0.013311        0.002608   \n",
       "7               0.000908                  0.001494   0.001823        0.001273   \n",
       "8               0.000688                  0.001101   0.003332        0.000885   \n",
       "9               0.001601                  0.004082   0.001134        0.001712   \n",
       "10              0.000761                  0.001747   0.002613        0.000519   \n",
       "11              0.000440                  0.000850   0.001402        0.000989   \n",
       "12              0.001005                  0.008191   0.017822        0.001421   \n",
       "13              0.000834                  0.002083   0.015680        0.000874   \n",
       "14              0.015184                  0.039437   0.005612        0.009877   \n",
       "15              0.000641                  0.001227   0.001284        0.000997   \n",
       "16              0.000862                  0.002367   0.002776        0.001433   \n",
       "17              0.005776                  0.038829   0.002545        0.002842   \n",
       "18              0.001144                  0.002067   0.003258        0.001024   \n",
       "19              0.008138                  0.019119   0.004378        0.004757   \n",
       "20              0.000430                  0.001038   0.002747        0.002063   \n",
       "21              0.000492                  0.000939   0.001440        0.001285   \n",
       "22              0.000779                  0.001473   0.006611        0.000611   \n",
       "23              0.000670                  0.001350   0.002225        0.000903   \n",
       "24              0.000970                  0.001267   0.002966        0.000712   \n",
       "25              0.000420                  0.001412   0.003820        0.000995   \n",
       "26              0.000385                  0.000738   0.001343        0.000925   \n",
       "27              0.000340                  0.000644   0.000918        0.001075   \n",
       "28              0.001058                  0.001151   0.001809        0.001345   \n",
       "29              0.000736                  0.001189   0.000743        0.000978   \n",
       "...                  ...                       ...        ...             ...   \n",
       "3408            0.023100                  0.013902   0.007583        0.007022   \n",
       "3409            0.014063                  0.009525   0.005612        0.005481   \n",
       "3410            0.011648                  0.010704   0.005133        0.006939   \n",
       "3411            0.000368                  0.000983   0.000742        0.001146   \n",
       "3412            0.003339                  0.003540   0.002563        0.002103   \n",
       "3413            0.009472                  0.008964   0.004154        0.004435   \n",
       "3414            0.012059                  0.010659   0.005927        0.005475   \n",
       "3415            0.002163                  0.002527   0.001358        0.004578   \n",
       "3416            0.004761                  0.008198   0.001231        0.013850   \n",
       "3417            0.003054                  0.012117   0.002335        0.102267   \n",
       "3418            0.000770                  0.004257   0.001026        0.011052   \n",
       "3419            0.000673                  0.001207   0.001907        0.001407   \n",
       "3420            0.001134                  0.002493   0.000613        0.005161   \n",
       "3421            0.007423                  0.003516   0.012355        0.035146   \n",
       "3422            0.003423                  0.001956   0.005505        0.001461   \n",
       "3423            0.000627                  0.000940   0.001246        0.001286   \n",
       "3424            0.000365                  0.000965   0.001233        0.002309   \n",
       "3425            0.000437                  0.000647   0.001400        0.000582   \n",
       "3426            0.000389                  0.000814   0.000753        0.001547   \n",
       "3427            0.000269                  0.000678   0.000603        0.001533   \n",
       "3428            0.000670                  0.001133   0.000854        0.001299   \n",
       "3429            0.000615                  0.000930   0.002467        0.000794   \n",
       "3430            0.001199                  0.002164   0.001512        0.002641   \n",
       "3431            0.030645                  0.145497   0.049902        0.075449   \n",
       "3432            0.150643                  0.063706   0.821862        0.922894   \n",
       "3433            0.021146                  0.045643   0.025759        0.998094   \n",
       "3434            0.001822                  0.003738   0.003088        0.001041   \n",
       "3435            0.119367                  0.438786   0.136498        0.983333   \n",
       "3436            0.086670                  0.314895   0.383018        0.103285   \n",
       "3437            0.020898                  0.041824   0.004765        0.013242   \n",
       "\n",
       "      Tax dispute  Value-added tax (VAT)  Withholding Tax  \n",
       "0        0.001213               0.001534         0.001609  \n",
       "1        0.002956               0.001092         0.001084  \n",
       "2        0.003071               0.085376         0.005087  \n",
       "3        0.000937               0.000955         0.001575  \n",
       "4        0.000937               0.000955         0.001575  \n",
       "5        0.004058               0.004306         0.002282  \n",
       "6        0.019606               0.170600         0.004676  \n",
       "7        0.001203               0.002801         0.001008  \n",
       "8        0.002034               0.001625         0.002067  \n",
       "9        0.000965               0.004292         0.001635  \n",
       "10       0.004579               0.003493         0.000532  \n",
       "11       0.000943               0.001078         0.001038  \n",
       "12       0.012643               0.003637         0.000822  \n",
       "13       0.013409               0.004898         0.001048  \n",
       "14       0.010459               0.209587         0.010247  \n",
       "15       0.002259               0.002123         0.000609  \n",
       "16       0.003122               0.001658         0.002170  \n",
       "17       0.001984               0.007417         0.002924  \n",
       "18       0.002646               0.003018         0.001134  \n",
       "19       0.006783               0.193170         0.008214  \n",
       "20       0.006576               0.001280         0.001421  \n",
       "21       0.001295               0.001091         0.000883  \n",
       "22       0.003243               0.001501         0.002887  \n",
       "23       0.002123               0.001167         0.000837  \n",
       "24       0.001912               0.002130         0.001550  \n",
       "25       0.002700               0.001343         0.000883  \n",
       "26       0.000822               0.000994         0.000913  \n",
       "27       0.001176               0.000527         0.000940  \n",
       "28       0.000750               0.001990         0.003224  \n",
       "29       0.000956               0.001288         0.001308  \n",
       "...           ...                    ...              ...  \n",
       "3408     0.016453               0.002008         0.021042  \n",
       "3409     0.009910               0.001581         0.019528  \n",
       "3410     0.013957               0.001351         0.008930  \n",
       "3411     0.000574               0.001016         0.001010  \n",
       "3412     0.003596               0.001174         0.002853  \n",
       "3413     0.013982               0.001164         0.010124  \n",
       "3414     0.012981               0.001463         0.009705  \n",
       "3415     0.001032               0.093785         0.003743  \n",
       "3416     0.001532               0.302574         0.003626  \n",
       "3417     0.019361               0.002749         0.377078  \n",
       "3418     0.000820               0.006286         0.003647  \n",
       "3419     0.001060               0.002373         0.001739  \n",
       "3420     0.000434               0.015360         0.009882  \n",
       "3421     0.001971               0.064351         0.205310  \n",
       "3422     0.001958               0.075672         0.010211  \n",
       "3423     0.001109               0.001271         0.001300  \n",
       "3424     0.000889               0.001107         0.001564  \n",
       "3425     0.001127               0.001673         0.001143  \n",
       "3426     0.000763               0.001036         0.000969  \n",
       "3427     0.000869               0.000547         0.001102  \n",
       "3428     0.000811               0.002333         0.001242  \n",
       "3429     0.001655               0.002363         0.001314  \n",
       "3430     0.000854               0.006529         0.001236  \n",
       "3431     0.016587               0.232692         0.008269  \n",
       "3432     0.108795               0.328709         0.019598  \n",
       "3433     0.008539               0.007195         0.014905  \n",
       "3434     0.004351               0.002704         0.001288  \n",
       "3435     0.024444               0.263066         0.018919  \n",
       "3436     0.024319               0.686949         0.020997  \n",
       "3437     0.001688               0.056590         0.003524  \n",
       "\n",
       "[3438 rows x 29 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cing12.html\t0.060398\t0.088003\t0.068467\t0.065018\t0.045323\t0.048387\t0.085089\t0.026707\t0.047605\t0.220333\n",
    "1\tExecutive summary\\n\\nOn 19 October 2018, the S...\ttax_controversy9.html\t0.240468\t0.194115\t0.105509\t0.113727\t0.035817\t0.034487\t0.071500\t0.028623\t0.038994\t0.062349\n",
    "2\tCompanies remain under pressure to respond to ...\ttax_transparency2.html\t0.036964\tresult = predict(model, DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Reference', 'text', '(Management of a) Tax function',\n",
       "       'Anti-Tax Avoidance Directive (ATAD)', 'Controlled Foreign Corporation',\n",
       "       'Corporate Tax', 'Country-by-Country Reporting', 'Digital Tax',\n",
       "       'Double Tax Treaty', 'EU Mandatory Disclosure Directive (DAC6)',\n",
       "       'Economic substance', 'General anti-avoidance rule (GAAR)',\n",
       "       'Goods and services tax (GST)', 'Interest deductibility', 'Local file',\n",
       "       'Mandatory disclosure rules', 'Master file',\n",
       "       'Multilateral Instrument (MLI)', 'Mutual agreement procedure',\n",
       "       'Permanent Establishment (PE)', 'Principal purpose test', 'Section 892',\n",
       "       'Sovereign Immunity', 'Tax Governance Framework', 'Tax audit',\n",
       "       'Tax compliance', 'Tax dispute', 'Value-added tax (VAT)',\n",
       "       'Withholding Tax'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombine_test(dataframe):\n",
    "    \"\"\"\n",
    "    Dataframe -> Dataframe\n",
    "    \n",
    "    Takes the results of a dataframe of results from a test dataset split into paragraphs\n",
    "    and recombines the predictions into document-level by taking the max\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_refs = dataframe.Reference.unique()\n",
    "    combine_list = []\n",
    "    test_labels = dataframe.columns[2:]\n",
    "\n",
    "    for unique_ref in unique_refs:\n",
    "        dict_for_ref = dict()\n",
    "        dict_for_ref[\"Reference\"] = unique_ref\n",
    "        dict_for_ref[\"text\"] = ' '.join(list(result[result[\"Reference\"]==unique_ref][\"text\"]))\n",
    "        for label in test_labels:\n",
    "            dict_for_ref[label] = result[result[\"Reference\"]==unique_ref][label].max()\n",
    "        combine_list.append(dict_for_ref)\n",
    "    return pd.DataFrame(combine_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombined_results = recombine_test(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>text</th>\n",
       "      <th>(Management of a) Tax function</th>\n",
       "      <th>Anti-Tax Avoidance Directive (ATAD)</th>\n",
       "      <th>Controlled Foreign Corporation</th>\n",
       "      <th>Corporate Tax</th>\n",
       "      <th>Country-by-Country Reporting</th>\n",
       "      <th>Digital Tax</th>\n",
       "      <th>Double Tax Treaty</th>\n",
       "      <th>EU Mandatory Disclosure Directive (DAC6)</th>\n",
       "      <th>...</th>\n",
       "      <th>Permanent Establishment (PE)</th>\n",
       "      <th>Principal purpose test</th>\n",
       "      <th>Section 892</th>\n",
       "      <th>Sovereign Immunity</th>\n",
       "      <th>Tax Governance Framework</th>\n",
       "      <th>Tax audit</th>\n",
       "      <th>Tax compliance</th>\n",
       "      <th>Tax dispute</th>\n",
       "      <th>Value-added tax (VAT)</th>\n",
       "      <th>Withholding Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>OECD/G20 Base Erosion and Profit Shifting Project</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>Addressing the Tax Challenges of the</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>Digitalisation of the Economy</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034298</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.085376</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Public Consultation Document</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>Public Consultation Document</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>Following a mandate by G20 Finance Ministers i...</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.031505</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.002282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>on BEPS, working through its Task Force on the...</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.957679</td>\n",
       "      <td>0.022958</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034356</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.012617</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.019606</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.004676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>Interim Report in March 2018, Tax Challenges A...</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.001008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>Report 2018. One of the important conclusions ...</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.002067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>review the impact of digitalisation on nexus a...</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.001635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>continue working together towards a final repo...</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>based long-term solution, with an update in 2019.</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>Since the delivery of the Interim Report, the ...</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>work and several proposals emerged that could ...</td>\n",
       "      <td>0.018148</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.064779</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.001048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>broader challenges arising from the digitalisa...</td>\n",
       "      <td>0.087101</td>\n",
       "      <td>0.021356</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.995179</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135562</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.039437</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.209587</td>\n",
       "      <td>0.010247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>BEPS issues. The work on these proposals is be...</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>basis; their examination does not represent a ...</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.002170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>Framework beyond exploring these proposals. In...</td>\n",
       "      <td>0.018927</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.002924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>agreed to hold a public consultation on possib...</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.001134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>from the digitalisation of the economy on 13 a...</td>\n",
       "      <td>0.039751</td>\n",
       "      <td>0.011471</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.976524</td>\n",
       "      <td>0.026930</td>\n",
       "      <td>0.009957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084866</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.193170</td>\n",
       "      <td>0.008214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>Conference Centre in Paris, France. The object...</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>opportunity to provide input early in the proc...</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>As part of this public consultation, this cons...</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.037626</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.002887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>discussed by the Inclusive Framework at a high...</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>public on a number of policy issues and techni...</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15</td>\n",
       "      <td>assist members of the Inclusive Framework in t...</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15</td>\n",
       "      <td>report to the G20 in 2020.</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15</td>\n",
       "      <td>Interested parties are invited to send their c...</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15</td>\n",
       "      <td>Comments should be sent by 1 March 2019 at the...</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.003224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>Word format (in order to facilitate their dist...</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.001308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>Another instance GAAR has been invoked relates...</td>\n",
       "      <td>0.049858</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.012544</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.021042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>Hitesh Gajaria, head of tax, KPMG India, belie...</td>\n",
       "      <td>0.036279</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.019528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>If companies are able to demonstrate commercia...</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.016512</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.008930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>Often, deals are structured in certain ways to...</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.001010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>But if it strikes a deal with a single buyer f...</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.010075</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.016076</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>There are apprehensions that given the sweepin...</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.009952</td>\n",
       "      <td>0.013719</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.010124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>With the advent of GAAR, the number of litiga...</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.011993</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.014087</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.009705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Chinese sales tax: bye-bye business tax</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.030789</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.093785</td>\n",
       "      <td>0.003743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>On March 5, 2016 the long awaited last stage o...</td>\n",
       "      <td>0.082455</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.028040</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.095444</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.008030</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.302574</td>\n",
       "      <td>0.003626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The business tax which had been applicable for...</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.634783</td>\n",
       "      <td>0.031122</td>\n",
       "      <td>0.068106</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.011462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.038627</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.102267</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.377078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>In particular at an international level the Ch...</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.613025</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.003647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>According to the previously submitted draft la...</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The 4 remaining sectors where business tax is ...</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.076567</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>0.009882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>For tax assessments of the cross-border servic...</td>\n",
       "      <td>0.075590</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.030423</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>0.035146</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.064351</td>\n",
       "      <td>0.205310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Should taxpayers with foreign loans have to wi...</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.019188</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.075672</td>\n",
       "      <td>0.010211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Can the input tax for necessary accommodation ...</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>If current office rental contracts expire in t...</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The inclusion of construction services in the ...</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>Regarding current contracts and transaction wh...</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.000969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>For current transactions not yet summarised by...</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>The current business model and the type of tra...</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>https://www.roedl.com/insights/china-sales-tax...</td>\n",
       "      <td>As the finalisation of the sales tax reform ca...</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>Opinion | Tech changing the face of indirect t...</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>Tax as a subject has undoubtedly gained priori...</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.083787</td>\n",
       "      <td>0.056272</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.054327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033279</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>0.145497</td>\n",
       "      <td>0.049902</td>\n",
       "      <td>0.075449</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.232692</td>\n",
       "      <td>0.008269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>This is primarily on account of the pace at wh...</td>\n",
       "      <td>0.793611</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.035558</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.088239</td>\n",
       "      <td>0.534807</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.032928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.084340</td>\n",
       "      <td>0.150643</td>\n",
       "      <td>0.063706</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.922894</td>\n",
       "      <td>0.108795</td>\n",
       "      <td>0.328709</td>\n",
       "      <td>0.019598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>With governments gearing up with technology an...</td>\n",
       "      <td>0.341052</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.021498</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.018346</td>\n",
       "      <td>0.021146</td>\n",
       "      <td>0.045643</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.998094</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.014905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>There are many examples of companies deploying...</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>However, organizations can no longer stop at j...</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.118482</td>\n",
       "      <td>0.078150</td>\n",
       "      <td>0.108975</td>\n",
       "      <td>0.293462</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>0.182493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053615</td>\n",
       "      <td>0.013594</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>0.119367</td>\n",
       "      <td>0.438786</td>\n",
       "      <td>0.136498</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.018919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>Organizations are not shy about embarking upon...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.093839</td>\n",
       "      <td>0.077183</td>\n",
       "      <td>0.070110</td>\n",
       "      <td>0.203103</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.106270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>0.055816</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.314895</td>\n",
       "      <td>0.383018</td>\n",
       "      <td>0.103285</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>0.686949</td>\n",
       "      <td>0.020997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>https://www.livemint.com/opinion/blogs/opinion...</td>\n",
       "      <td>As in every field, evolution and innovation wi...</td>\n",
       "      <td>0.651038</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.251662</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.019672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.041824</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.056590</td>\n",
       "      <td>0.003524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3438 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "0                                                    15   \n",
       "1                                                    15   \n",
       "2                                                    15   \n",
       "3                                                    15   \n",
       "4                                                    15   \n",
       "5                                                    15   \n",
       "6                                                    15   \n",
       "7                                                    15   \n",
       "8                                                    15   \n",
       "9                                                    15   \n",
       "10                                                   15   \n",
       "11                                                   15   \n",
       "12                                                   15   \n",
       "13                                                   15   \n",
       "14                                                   15   \n",
       "15                                                   15   \n",
       "16                                                   15   \n",
       "17                                                   15   \n",
       "18                                                   15   \n",
       "19                                                   15   \n",
       "20                                                   15   \n",
       "21                                                   15   \n",
       "22                                                   15   \n",
       "23                                                   15   \n",
       "24                                                   15   \n",
       "25                                                   15   \n",
       "26                                                   15   \n",
       "27                                                   15   \n",
       "28                                                   15   \n",
       "29                                                   15   \n",
       "...                                                 ...   \n",
       "3408  https://economictimes.indiatimes.com/news/econ...   \n",
       "3409  https://economictimes.indiatimes.com/news/econ...   \n",
       "3410  https://economictimes.indiatimes.com/news/econ...   \n",
       "3411  https://economictimes.indiatimes.com/news/econ...   \n",
       "3412  https://economictimes.indiatimes.com/news/econ...   \n",
       "3413  https://economictimes.indiatimes.com/news/econ...   \n",
       "3414  https://economictimes.indiatimes.com/news/econ...   \n",
       "3415  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3416  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3417  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3418  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3419  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3420  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3421  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3422  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3423  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3424  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3425  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3426  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3427  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3428  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3429  https://www.roedl.com/insights/china-sales-tax...   \n",
       "3430  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3431  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3432  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3433  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3434  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3435  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3436  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "3437  https://www.livemint.com/opinion/blogs/opinion...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     OECD/G20 Base Erosion and Profit Shifting Project   \n",
       "1                  Addressing the Tax Challenges of the   \n",
       "2                         Digitalisation of the Economy   \n",
       "3                          Public Consultation Document   \n",
       "4                          Public Consultation Document   \n",
       "5     Following a mandate by G20 Finance Ministers i...   \n",
       "6     on BEPS, working through its Task Force on the...   \n",
       "7     Interim Report in March 2018, Tax Challenges A...   \n",
       "8     Report 2018. One of the important conclusions ...   \n",
       "9     review the impact of digitalisation on nexus a...   \n",
       "10    continue working together towards a final repo...   \n",
       "11    based long-term solution, with an update in 2019.   \n",
       "12    Since the delivery of the Interim Report, the ...   \n",
       "13    work and several proposals emerged that could ...   \n",
       "14    broader challenges arising from the digitalisa...   \n",
       "15    BEPS issues. The work on these proposals is be...   \n",
       "16    basis; their examination does not represent a ...   \n",
       "17    Framework beyond exploring these proposals. In...   \n",
       "18    agreed to hold a public consultation on possib...   \n",
       "19    from the digitalisation of the economy on 13 a...   \n",
       "20    Conference Centre in Paris, France. The object...   \n",
       "21    opportunity to provide input early in the proc...   \n",
       "22    As part of this public consultation, this cons...   \n",
       "23    discussed by the Inclusive Framework at a high...   \n",
       "24    public on a number of policy issues and techni...   \n",
       "25    assist members of the Inclusive Framework in t...   \n",
       "26                           report to the G20 in 2020.   \n",
       "27    Interested parties are invited to send their c...   \n",
       "28    Comments should be sent by 1 March 2019 at the...   \n",
       "29    Word format (in order to facilitate their dist...   \n",
       "...                                                 ...   \n",
       "3408  Another instance GAAR has been invoked relates...   \n",
       "3409  Hitesh Gajaria, head of tax, KPMG India, belie...   \n",
       "3410  If companies are able to demonstrate commercia...   \n",
       "3411  Often, deals are structured in certain ways to...   \n",
       "3412  But if it strikes a deal with a single buyer f...   \n",
       "3413  There are apprehensions that given the sweepin...   \n",
       "3414  With the advent of GAAR, the number of litiga...   \n",
       "3415            Chinese sales tax: bye-bye business tax   \n",
       "3416  On March 5, 2016 the long awaited last stage o...   \n",
       "3417  The business tax which had been applicable for...   \n",
       "3418  In particular at an international level the Ch...   \n",
       "3419  According to the previously submitted draft la...   \n",
       "3420  The 4 remaining sectors where business tax is ...   \n",
       "3421  For tax assessments of the cross-border servic...   \n",
       "3422  Should taxpayers with foreign loans have to wi...   \n",
       "3423  Can the input tax for necessary accommodation ...   \n",
       "3424  If current office rental contracts expire in t...   \n",
       "3425  The inclusion of construction services in the ...   \n",
       "3426  Regarding current contracts and transaction wh...   \n",
       "3427  For current transactions not yet summarised by...   \n",
       "3428  The current business model and the type of tra...   \n",
       "3429  As the finalisation of the sales tax reform ca...   \n",
       "3430  Opinion | Tech changing the face of indirect t...   \n",
       "3431  Tax as a subject has undoubtedly gained priori...   \n",
       "3432  This is primarily on account of the pace at wh...   \n",
       "3433  With governments gearing up with technology an...   \n",
       "3434  There are many examples of companies deploying...   \n",
       "3435  However, organizations can no longer stop at j...   \n",
       "3436  Organizations are not shy about embarking upon...   \n",
       "3437  As in every field, evolution and innovation wi...   \n",
       "\n",
       "      (Management of a) Tax function  Anti-Tax Avoidance Directive (ATAD)  \\\n",
       "0                           0.003044                             0.000463   \n",
       "1                           0.002656                             0.001092   \n",
       "2                           0.053961                             0.004193   \n",
       "3                           0.001372                             0.001472   \n",
       "4                           0.001372                             0.001472   \n",
       "5                           0.008591                             0.005943   \n",
       "6                           0.026746                             0.010515   \n",
       "7                           0.003487                             0.000647   \n",
       "8                           0.002212                             0.000510   \n",
       "9                           0.006703                             0.001405   \n",
       "10                          0.005002                             0.000459   \n",
       "11                          0.001820                             0.000570   \n",
       "12                          0.004728                             0.006959   \n",
       "13                          0.018148                             0.000855   \n",
       "14                          0.087101                             0.021356   \n",
       "15                          0.002669                             0.000750   \n",
       "16                          0.004710                             0.000640   \n",
       "17                          0.018927                             0.007333   \n",
       "18                          0.004123                             0.000623   \n",
       "19                          0.039751                             0.011471   \n",
       "20                          0.005230                             0.000684   \n",
       "21                          0.003124                             0.000491   \n",
       "22                          0.000649                             0.037626   \n",
       "23                          0.001420                             0.000990   \n",
       "24                          0.001682                             0.001752   \n",
       "25                          0.003907                             0.001579   \n",
       "26                          0.001605                             0.000699   \n",
       "27                          0.001021                             0.001048   \n",
       "28                          0.002379                             0.000434   \n",
       "29                          0.002200                             0.000497   \n",
       "...                              ...                                  ...   \n",
       "3408                        0.049858                             0.014727   \n",
       "3409                        0.036279                             0.009910   \n",
       "3410                        0.041070                             0.010453   \n",
       "3411                        0.002463                             0.000439   \n",
       "3412                        0.011852                             0.006900   \n",
       "3413                        0.032057                             0.011183   \n",
       "3414                        0.047442                             0.013554   \n",
       "3415                        0.022064                             0.000744   \n",
       "3416                        0.082455                             0.000954   \n",
       "3417                        0.005168                             0.017071   \n",
       "3418                        0.017695                             0.001063   \n",
       "3419                        0.003515                             0.000310   \n",
       "3420                        0.023776                             0.000311   \n",
       "3421                        0.075590                             0.000403   \n",
       "3422                        0.014696                             0.000402   \n",
       "3423                        0.002632                             0.000505   \n",
       "3424                        0.003814                             0.000584   \n",
       "3425                        0.001403                             0.000872   \n",
       "3426                        0.002045                             0.000529   \n",
       "3427                        0.000963                             0.000786   \n",
       "3428                        0.003064                             0.000482   \n",
       "3429                        0.001708                             0.000816   \n",
       "3430                        0.010861                             0.000474   \n",
       "3431                        0.999273                             0.002811   \n",
       "3432                        0.793611                             0.002164   \n",
       "3433                        0.341052                             0.001394   \n",
       "3434                        0.004200                             0.000469   \n",
       "3435                        0.999654                             0.005228   \n",
       "3436                        0.999639                             0.004059   \n",
       "3437                        0.651038                             0.000806   \n",
       "\n",
       "      Controlled Foreign Corporation  Corporate Tax  \\\n",
       "0                           0.000574       0.001024   \n",
       "1                           0.000343       0.001406   \n",
       "2                           0.004587       0.006153   \n",
       "3                           0.000378       0.001080   \n",
       "4                           0.000378       0.001080   \n",
       "5                           0.001007       0.006257   \n",
       "6                           0.004537       0.005921   \n",
       "7                           0.000546       0.000813   \n",
       "8                           0.000543       0.000837   \n",
       "9                           0.001091       0.001370   \n",
       "10                          0.000604       0.000737   \n",
       "11                          0.000335       0.001018   \n",
       "12                          0.000418       0.005379   \n",
       "13                          0.000575       0.002064   \n",
       "14                          0.009792       0.016447   \n",
       "15                          0.000411       0.001218   \n",
       "16                          0.000505       0.001180   \n",
       "17                          0.002821       0.003611   \n",
       "18                          0.000665       0.000808   \n",
       "19                          0.007631       0.013311   \n",
       "20                          0.000313       0.000523   \n",
       "21                          0.000305       0.000705   \n",
       "22                          0.000511       0.005815   \n",
       "23                          0.000428       0.000895   \n",
       "24                          0.000568       0.001608   \n",
       "25                          0.000399       0.000968   \n",
       "26                          0.000291       0.001000   \n",
       "27                          0.000225       0.000975   \n",
       "28                          0.001118       0.001068   \n",
       "29                          0.000579       0.000736   \n",
       "...                              ...            ...   \n",
       "3408                        0.009352       0.012406   \n",
       "3409                        0.006204       0.010076   \n",
       "3410                        0.005262       0.008626   \n",
       "3411                        0.000336       0.001731   \n",
       "3412                        0.002602       0.002903   \n",
       "3413                        0.004470       0.011087   \n",
       "3414                        0.005299       0.008082   \n",
       "3415                        0.003236       0.014245   \n",
       "3416                        0.007778       0.028040   \n",
       "3417                        0.017372       0.634783   \n",
       "3418                        0.001016       0.613025   \n",
       "3419                        0.000548       0.000893   \n",
       "3420                        0.001335       0.076567   \n",
       "3421                        0.006477       0.003338   \n",
       "3422                        0.003005       0.007271   \n",
       "3423                        0.000346       0.001131   \n",
       "3424                        0.000303       0.001284   \n",
       "3425                        0.000308       0.001768   \n",
       "3426                        0.000308       0.001563   \n",
       "3427                        0.000253       0.001174   \n",
       "3428                        0.000512       0.001149   \n",
       "3429                        0.000391       0.000759   \n",
       "3430                        0.000993       0.001270   \n",
       "3431                        0.038319       0.045543   \n",
       "3432                        0.035558       0.008935   \n",
       "3433                        0.009154       0.012293   \n",
       "3434                        0.001151       0.001222   \n",
       "3435                        0.118482       0.078150   \n",
       "3436                        0.093839       0.077183   \n",
       "3437                        0.028906       0.002955   \n",
       "\n",
       "      Country-by-Country Reporting  Digital Tax  Double Tax Treaty  \\\n",
       "0                         0.000703     0.002957           0.001392   \n",
       "1                         0.000622     0.003077           0.002149   \n",
       "2                         0.001270     0.876471           0.007152   \n",
       "3                         0.000919     0.002173           0.001455   \n",
       "4                         0.000919     0.002173           0.001455   \n",
       "5                         0.001127     0.031505           0.005491   \n",
       "6                         0.004654     0.957679           0.022958   \n",
       "7                         0.000546     0.003020           0.001132   \n",
       "8                         0.002185     0.001370           0.001818   \n",
       "9                         0.000573     0.062245           0.002521   \n",
       "10                        0.003870     0.002098           0.001318   \n",
       "11                        0.001003     0.001688           0.001237   \n",
       "12                        0.016447     0.013062           0.003988   \n",
       "13                        0.001958     0.064779           0.003269   \n",
       "14                        0.004307     0.995179           0.030557   \n",
       "15                        0.000701     0.004477           0.001988   \n",
       "16                        0.001001     0.009804           0.003376   \n",
       "17                        0.000617     0.010325           0.003320   \n",
       "18                        0.000518     0.008545           0.002037   \n",
       "19                        0.002523     0.976524           0.026930   \n",
       "20                        0.002188     0.009711           0.001944   \n",
       "21                        0.000841     0.002449           0.001232   \n",
       "22                        0.006393     0.006750           0.003966   \n",
       "23                        0.001051     0.003640           0.001894   \n",
       "24                        0.000487     0.001628           0.002462   \n",
       "25                        0.004840     0.005626           0.001621   \n",
       "26                        0.002148     0.001496           0.001165   \n",
       "27                        0.001373     0.001317           0.001567   \n",
       "28                        0.001523     0.002307           0.001720   \n",
       "29                        0.000989     0.001981           0.001587   \n",
       "...                            ...          ...                ...   \n",
       "3408                      0.012544     0.010344           0.022188   \n",
       "3409                      0.008426     0.008658           0.017370   \n",
       "3410                      0.012070     0.010353           0.016512   \n",
       "3411                      0.001274     0.001072           0.001251   \n",
       "3412                      0.004254     0.003436           0.010075   \n",
       "3413                      0.011582     0.009952           0.013719   \n",
       "3414                      0.013473     0.011993           0.012327   \n",
       "3415                      0.001261     0.030789           0.002825   \n",
       "3416                      0.003912     0.095444           0.003877   \n",
       "3417                      0.031122     0.068106           0.998519   \n",
       "3418                      0.002337     0.003725           0.002846   \n",
       "3419                      0.000605     0.002521           0.001302   \n",
       "3420                      0.003390     0.007648           0.003354   \n",
       "3421                      0.001719     0.068935           0.030423   \n",
       "3422                      0.001090     0.019188           0.002464   \n",
       "3423                      0.000457     0.001706           0.001224   \n",
       "3424                      0.001154     0.001843           0.001491   \n",
       "3425                      0.001151     0.001788           0.001230   \n",
       "3426                      0.000815     0.001487           0.001349   \n",
       "3427                      0.001023     0.000960           0.001830   \n",
       "3428                      0.000491     0.002972           0.001529   \n",
       "3429                      0.001106     0.002494           0.001077   \n",
       "3430                      0.000770     0.007416           0.001275   \n",
       "3431                      0.083787     0.056272           0.004834   \n",
       "3432                      0.088239     0.534807           0.003444   \n",
       "3433                      0.010605     0.021498           0.006212   \n",
       "3434                      0.004758     0.003949           0.002286   \n",
       "3435                      0.108975     0.293462           0.009792   \n",
       "3436                      0.070110     0.203103           0.004800   \n",
       "3437                      0.000828     0.251662           0.002871   \n",
       "\n",
       "      EU Mandatory Disclosure Directive (DAC6)  ...  \\\n",
       "0                                     0.000712  ...   \n",
       "1                                     0.000899  ...   \n",
       "2                                     0.005327  ...   \n",
       "3                                     0.000690  ...   \n",
       "4                                     0.000690  ...   \n",
       "5                                     0.005138  ...   \n",
       "6                                     0.007104  ...   \n",
       "7                                     0.001162  ...   \n",
       "8                                     0.001059  ...   \n",
       "9                                     0.002040  ...   \n",
       "10                                    0.001572  ...   \n",
       "11                                    0.000597  ...   \n",
       "12                                    0.003830  ...   \n",
       "13                                    0.000917  ...   \n",
       "14                                    0.013811  ...   \n",
       "15                                    0.000876  ...   \n",
       "16                                    0.001266  ...   \n",
       "17                                    0.012314  ...   \n",
       "18                                    0.001569  ...   \n",
       "19                                    0.009957  ...   \n",
       "20                                    0.001119  ...   \n",
       "21                                    0.000711  ...   \n",
       "22                                    0.001518  ...   \n",
       "23                                    0.001040  ...   \n",
       "24                                    0.001972  ...   \n",
       "25                                    0.000812  ...   \n",
       "26                                    0.000707  ...   \n",
       "27                                    0.000524  ...   \n",
       "28                                    0.001626  ...   \n",
       "29                                    0.001094  ...   \n",
       "...                                        ...  ...   \n",
       "3408                                  0.003079  ...   \n",
       "3409                                  0.002080  ...   \n",
       "3410                                  0.002434  ...   \n",
       "3411                                  0.000677  ...   \n",
       "3412                                  0.001693  ...   \n",
       "3413                                  0.001689  ...   \n",
       "3414                                  0.002397  ...   \n",
       "3415                                  0.001899  ...   \n",
       "3416                                  0.005237  ...   \n",
       "3417                                  0.011462  ...   \n",
       "3418                                  0.000482  ...   \n",
       "3419                                  0.000907  ...   \n",
       "3420                                  0.000753  ...   \n",
       "3421                                  0.011133  ...   \n",
       "3422                                  0.001516  ...   \n",
       "3423                                  0.000805  ...   \n",
       "3424                                  0.000773  ...   \n",
       "3425                                  0.000496  ...   \n",
       "3426                                  0.000618  ...   \n",
       "3427                                  0.000579  ...   \n",
       "3428                                  0.000941  ...   \n",
       "3429                                  0.000721  ...   \n",
       "3430                                  0.001667  ...   \n",
       "3431                                  0.054327  ...   \n",
       "3432                                  0.032928  ...   \n",
       "3433                                  0.019391  ...   \n",
       "3434                                  0.002011  ...   \n",
       "3435                                  0.182493  ...   \n",
       "3436                                  0.106270  ...   \n",
       "3437                                  0.019672  ...   \n",
       "\n",
       "      Permanent Establishment (PE)  Principal purpose test  Section 892  \\\n",
       "0                         0.001248                0.000536     0.000602   \n",
       "1                         0.001788                0.000371     0.000414   \n",
       "2                         0.034298                0.003201     0.002622   \n",
       "3                         0.000704                0.000393     0.000497   \n",
       "4                         0.000704                0.000393     0.000497   \n",
       "5                         0.003537                0.000819     0.000840   \n",
       "6                         0.034356                0.008705     0.004044   \n",
       "7                         0.002301                0.000729     0.000503   \n",
       "8                         0.001090                0.000962     0.001185   \n",
       "9                         0.001443                0.000641     0.001172   \n",
       "10                        0.001917                0.001507     0.000678   \n",
       "11                        0.000793                0.000478     0.000477   \n",
       "12                        0.002095                0.000831     0.001497   \n",
       "13                        0.001926                0.000719     0.000578   \n",
       "14                        0.135562                0.008460     0.006442   \n",
       "15                        0.001754                0.000633     0.000425   \n",
       "16                        0.001176                0.000620     0.000887   \n",
       "17                        0.002232                0.001633     0.001928   \n",
       "18                        0.001679                0.000636     0.000757   \n",
       "19                        0.084866                0.007342     0.003518   \n",
       "20                        0.001692                0.000480     0.000495   \n",
       "21                        0.001088                0.000444     0.000486   \n",
       "22                        0.001396                0.000830     0.000781   \n",
       "23                        0.000847                0.000493     0.000651   \n",
       "24                        0.002631                0.000823     0.000380   \n",
       "25                        0.000587                0.000436     0.000860   \n",
       "26                        0.000781                0.000476     0.000556   \n",
       "27                        0.000811                0.000400     0.000437   \n",
       "28                        0.001158                0.000902     0.001387   \n",
       "29                        0.001632                0.000646     0.000585   \n",
       "...                            ...                     ...          ...   \n",
       "3408                      0.005260                0.027094     0.003762   \n",
       "3409                      0.004333                0.018520     0.002583   \n",
       "3410                      0.004392                0.017293     0.002611   \n",
       "3411                      0.000818                0.000562     0.000418   \n",
       "3412                      0.002745                0.016076     0.000914   \n",
       "3413                      0.003614                0.012419     0.002584   \n",
       "3414                      0.003808                0.014087     0.002741   \n",
       "3415                      0.008888                0.002547     0.001741   \n",
       "3416                      0.018097                0.008030     0.005391   \n",
       "3417                      0.106869                0.038627     0.003945   \n",
       "3418                      0.012696                0.001150     0.000945   \n",
       "3419                      0.001074                0.000619     0.000584   \n",
       "3420                      0.010018                0.002217     0.001443   \n",
       "3421                      0.010787                0.003028     0.005475   \n",
       "3422                      0.002384                0.001850     0.002931   \n",
       "3423                      0.001853                0.000488     0.000370   \n",
       "3424                      0.001763                0.000407     0.000553   \n",
       "3425                      0.000906                0.000473     0.000468   \n",
       "3426                      0.001258                0.000455     0.000360   \n",
       "3427                      0.000831                0.000423     0.000341   \n",
       "3428                      0.002087                0.000685     0.000476   \n",
       "3429                      0.001117                0.000657     0.000604   \n",
       "3430                      0.001521                0.000661     0.000975   \n",
       "3431                      0.033279                0.015827     0.024430   \n",
       "3432                      0.012902                0.023008     0.084340   \n",
       "3433                      0.013384                0.002195     0.018346   \n",
       "3434                      0.000441                0.001228     0.003636   \n",
       "3435                      0.053615                0.013594     0.071919   \n",
       "3436                      0.032992                0.025257     0.055816   \n",
       "3437                      0.004141                0.003823     0.009188   \n",
       "\n",
       "      Sovereign Immunity  Tax Governance Framework  Tax audit  Tax compliance  \\\n",
       "0               0.000585                  0.001071   0.001626        0.001138   \n",
       "1               0.000652                  0.001134   0.003130        0.001982   \n",
       "2               0.006448                  0.012398   0.002304        0.003404   \n",
       "3               0.000459                  0.000933   0.001599        0.000968   \n",
       "4               0.000459                  0.000933   0.001599        0.000968   \n",
       "5               0.001499                  0.011445   0.002681        0.002059   \n",
       "6               0.006191                  0.012617   0.013311        0.002608   \n",
       "7               0.000908                  0.001494   0.001823        0.001273   \n",
       "8               0.000688                  0.001101   0.003332        0.000885   \n",
       "9               0.001601                  0.004082   0.001134        0.001712   \n",
       "10              0.000761                  0.001747   0.002613        0.000519   \n",
       "11              0.000440                  0.000850   0.001402        0.000989   \n",
       "12              0.001005                  0.008191   0.017822        0.001421   \n",
       "13              0.000834                  0.002083   0.015680        0.000874   \n",
       "14              0.015184                  0.039437   0.005612        0.009877   \n",
       "15              0.000641                  0.001227   0.001284        0.000997   \n",
       "16              0.000862                  0.002367   0.002776        0.001433   \n",
       "17              0.005776                  0.038829   0.002545        0.002842   \n",
       "18              0.001144                  0.002067   0.003258        0.001024   \n",
       "19              0.008138                  0.019119   0.004378        0.004757   \n",
       "20              0.000430                  0.001038   0.002747        0.002063   \n",
       "21              0.000492                  0.000939   0.001440        0.001285   \n",
       "22              0.000779                  0.001473   0.006611        0.000611   \n",
       "23              0.000670                  0.001350   0.002225        0.000903   \n",
       "24              0.000970                  0.001267   0.002966        0.000712   \n",
       "25              0.000420                  0.001412   0.003820        0.000995   \n",
       "26              0.000385                  0.000738   0.001343        0.000925   \n",
       "27              0.000340                  0.000644   0.000918        0.001075   \n",
       "28              0.001058                  0.001151   0.001809        0.001345   \n",
       "29              0.000736                  0.001189   0.000743        0.000978   \n",
       "...                  ...                       ...        ...             ...   \n",
       "3408            0.023100                  0.013902   0.007583        0.007022   \n",
       "3409            0.014063                  0.009525   0.005612        0.005481   \n",
       "3410            0.011648                  0.010704   0.005133        0.006939   \n",
       "3411            0.000368                  0.000983   0.000742        0.001146   \n",
       "3412            0.003339                  0.003540   0.002563        0.002103   \n",
       "3413            0.009472                  0.008964   0.004154        0.004435   \n",
       "3414            0.012059                  0.010659   0.005927        0.005475   \n",
       "3415            0.002163                  0.002527   0.001358        0.004578   \n",
       "3416            0.004761                  0.008198   0.001231        0.013850   \n",
       "3417            0.003054                  0.012117   0.002335        0.102267   \n",
       "3418            0.000770                  0.004257   0.001026        0.011052   \n",
       "3419            0.000673                  0.001207   0.001907        0.001407   \n",
       "3420            0.001134                  0.002493   0.000613        0.005161   \n",
       "3421            0.007423                  0.003516   0.012355        0.035146   \n",
       "3422            0.003423                  0.001956   0.005505        0.001461   \n",
       "3423            0.000627                  0.000940   0.001246        0.001286   \n",
       "3424            0.000365                  0.000965   0.001233        0.002309   \n",
       "3425            0.000437                  0.000647   0.001400        0.000582   \n",
       "3426            0.000389                  0.000814   0.000753        0.001547   \n",
       "3427            0.000269                  0.000678   0.000603        0.001533   \n",
       "3428            0.000670                  0.001133   0.000854        0.001299   \n",
       "3429            0.000615                  0.000930   0.002467        0.000794   \n",
       "3430            0.001199                  0.002164   0.001512        0.002641   \n",
       "3431            0.030645                  0.145497   0.049902        0.075449   \n",
       "3432            0.150643                  0.063706   0.821862        0.922894   \n",
       "3433            0.021146                  0.045643   0.025759        0.998094   \n",
       "3434            0.001822                  0.003738   0.003088        0.001041   \n",
       "3435            0.119367                  0.438786   0.136498        0.983333   \n",
       "3436            0.086670                  0.314895   0.383018        0.103285   \n",
       "3437            0.020898                  0.041824   0.004765        0.013242   \n",
       "\n",
       "      Tax dispute  Value-added tax (VAT)  Withholding Tax  \n",
       "0        0.001213               0.001534         0.001609  \n",
       "1        0.002956               0.001092         0.001084  \n",
       "2        0.003071               0.085376         0.005087  \n",
       "3        0.000937               0.000955         0.001575  \n",
       "4        0.000937               0.000955         0.001575  \n",
       "5        0.004058               0.004306         0.002282  \n",
       "6        0.019606               0.170600         0.004676  \n",
       "7        0.001203               0.002801         0.001008  \n",
       "8        0.002034               0.001625         0.002067  \n",
       "9        0.000965               0.004292         0.001635  \n",
       "10       0.004579               0.003493         0.000532  \n",
       "11       0.000943               0.001078         0.001038  \n",
       "12       0.012643               0.003637         0.000822  \n",
       "13       0.013409               0.004898         0.001048  \n",
       "14       0.010459               0.209587         0.010247  \n",
       "15       0.002259               0.002123         0.000609  \n",
       "16       0.003122               0.001658         0.002170  \n",
       "17       0.001984               0.007417         0.002924  \n",
       "18       0.002646               0.003018         0.001134  \n",
       "19       0.006783               0.193170         0.008214  \n",
       "20       0.006576               0.001280         0.001421  \n",
       "21       0.001295               0.001091         0.000883  \n",
       "22       0.003243               0.001501         0.002887  \n",
       "23       0.002123               0.001167         0.000837  \n",
       "24       0.001912               0.002130         0.001550  \n",
       "25       0.002700               0.001343         0.000883  \n",
       "26       0.000822               0.000994         0.000913  \n",
       "27       0.001176               0.000527         0.000940  \n",
       "28       0.000750               0.001990         0.003224  \n",
       "29       0.000956               0.001288         0.001308  \n",
       "...           ...                    ...              ...  \n",
       "3408     0.016453               0.002008         0.021042  \n",
       "3409     0.009910               0.001581         0.019528  \n",
       "3410     0.013957               0.001351         0.008930  \n",
       "3411     0.000574               0.001016         0.001010  \n",
       "3412     0.003596               0.001174         0.002853  \n",
       "3413     0.013982               0.001164         0.010124  \n",
       "3414     0.012981               0.001463         0.009705  \n",
       "3415     0.001032               0.093785         0.003743  \n",
       "3416     0.001532               0.302574         0.003626  \n",
       "3417     0.019361               0.002749         0.377078  \n",
       "3418     0.000820               0.006286         0.003647  \n",
       "3419     0.001060               0.002373         0.001739  \n",
       "3420     0.000434               0.015360         0.009882  \n",
       "3421     0.001971               0.064351         0.205310  \n",
       "3422     0.001958               0.075672         0.010211  \n",
       "3423     0.001109               0.001271         0.001300  \n",
       "3424     0.000889               0.001107         0.001564  \n",
       "3425     0.001127               0.001673         0.001143  \n",
       "3426     0.000763               0.001036         0.000969  \n",
       "3427     0.000869               0.000547         0.001102  \n",
       "3428     0.000811               0.002333         0.001242  \n",
       "3429     0.001655               0.002363         0.001314  \n",
       "3430     0.000854               0.006529         0.001236  \n",
       "3431     0.016587               0.232692         0.008269  \n",
       "3432     0.108795               0.328709         0.019598  \n",
       "3433     0.008539               0.007195         0.014905  \n",
       "3434     0.004351               0.002704         0.001288  \n",
       "3435     0.024444               0.263066         0.018919  \n",
       "3436     0.024319               0.686949         0.020997  \n",
       "3437     0.001688               0.056590         0.003524  \n",
       "\n",
       "[3438 rows x 29 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombined_results = recombined_results[label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Management of a) Tax function</th>\n",
       "      <th>Anti-Tax Avoidance Directive (ATAD)</th>\n",
       "      <th>Controlled Foreign Corporation</th>\n",
       "      <th>Corporate Tax</th>\n",
       "      <th>Country-by-Country Reporting</th>\n",
       "      <th>Digital Tax</th>\n",
       "      <th>Double Tax Treaty</th>\n",
       "      <th>EU Mandatory Disclosure Directive (DAC6)</th>\n",
       "      <th>Economic substance</th>\n",
       "      <th>General anti-avoidance rule (GAAR)</th>\n",
       "      <th>...</th>\n",
       "      <th>Permanent Establishment (PE)</th>\n",
       "      <th>Principal purpose test</th>\n",
       "      <th>Section 892</th>\n",
       "      <th>Sovereign Immunity</th>\n",
       "      <th>Tax Governance Framework</th>\n",
       "      <th>Tax audit</th>\n",
       "      <th>Tax compliance</th>\n",
       "      <th>Tax dispute</th>\n",
       "      <th>Value-added tax (VAT)</th>\n",
       "      <th>Withholding Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.565863</td>\n",
       "      <td>0.070124</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>0.392442</td>\n",
       "      <td>0.993504</td>\n",
       "      <td>0.999539</td>\n",
       "      <td>0.999217</td>\n",
       "      <td>0.116845</td>\n",
       "      <td>0.999112</td>\n",
       "      <td>0.140802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998668</td>\n",
       "      <td>0.069137</td>\n",
       "      <td>0.607566</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.706224</td>\n",
       "      <td>0.999475</td>\n",
       "      <td>0.136711</td>\n",
       "      <td>0.998861</td>\n",
       "      <td>0.409591</td>\n",
       "      <td>0.999835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.237646</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>0.295598</td>\n",
       "      <td>0.999770</td>\n",
       "      <td>0.658533</td>\n",
       "      <td>0.995187</td>\n",
       "      <td>0.334760</td>\n",
       "      <td>0.436822</td>\n",
       "      <td>0.998501</td>\n",
       "      <td>0.451808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968165</td>\n",
       "      <td>0.093563</td>\n",
       "      <td>0.070805</td>\n",
       "      <td>0.092779</td>\n",
       "      <td>0.052677</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.285971</td>\n",
       "      <td>0.588859</td>\n",
       "      <td>0.963702</td>\n",
       "      <td>0.999671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149199</td>\n",
       "      <td>0.996306</td>\n",
       "      <td>0.318034</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.203927</td>\n",
       "      <td>0.998625</td>\n",
       "      <td>0.274437</td>\n",
       "      <td>0.169203</td>\n",
       "      <td>0.994047</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997990</td>\n",
       "      <td>0.031619</td>\n",
       "      <td>0.955607</td>\n",
       "      <td>0.186714</td>\n",
       "      <td>0.046119</td>\n",
       "      <td>0.375709</td>\n",
       "      <td>0.999481</td>\n",
       "      <td>0.047518</td>\n",
       "      <td>0.252601</td>\n",
       "      <td>0.999754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064799</td>\n",
       "      <td>0.079855</td>\n",
       "      <td>0.046066</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>0.034842</td>\n",
       "      <td>0.109568</td>\n",
       "      <td>0.396354</td>\n",
       "      <td>0.030295</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019716</td>\n",
       "      <td>0.046870</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.281438</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>0.982878</td>\n",
       "      <td>0.039796</td>\n",
       "      <td>0.058485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.108322</td>\n",
       "      <td>0.026580</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.999678</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.045231</td>\n",
       "      <td>0.024651</td>\n",
       "      <td>0.035223</td>\n",
       "      <td>0.102719</td>\n",
       "      <td>0.006786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.026678</td>\n",
       "      <td>0.083684</td>\n",
       "      <td>0.047255</td>\n",
       "      <td>0.023218</td>\n",
       "      <td>0.025705</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.919066</td>\n",
       "      <td>0.037344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (Management of a) Tax function  Anti-Tax Avoidance Directive (ATAD)  \\\n",
       "0                        0.565863                             0.070124   \n",
       "1                        0.237646                             0.590536   \n",
       "2                        0.149199                             0.996306   \n",
       "3                        0.064799                             0.079855   \n",
       "4                        0.108322                             0.026580   \n",
       "\n",
       "   Controlled Foreign Corporation  Corporate Tax  \\\n",
       "0                        0.050526       0.392442   \n",
       "1                        0.295598       0.999770   \n",
       "2                        0.318034       0.999069   \n",
       "3                        0.046066       0.013134   \n",
       "4                        0.062988       0.999678   \n",
       "\n",
       "   Country-by-Country Reporting  Digital Tax  Double Tax Treaty  \\\n",
       "0                      0.993504     0.999539           0.999217   \n",
       "1                      0.658533     0.995187           0.334760   \n",
       "2                      0.203927     0.998625           0.274437   \n",
       "3                      0.034842     0.109568           0.396354   \n",
       "4                      0.021008     0.045231           0.024651   \n",
       "\n",
       "   EU Mandatory Disclosure Directive (DAC6)  Economic substance  \\\n",
       "0                                  0.116845            0.999112   \n",
       "1                                  0.436822            0.998501   \n",
       "2                                  0.169203            0.994047   \n",
       "3                                  0.030295            0.021112   \n",
       "4                                  0.035223            0.102719   \n",
       "\n",
       "   General anti-avoidance rule (GAAR)  ...  Permanent Establishment (PE)  \\\n",
       "0                            0.140802  ...                      0.998668   \n",
       "1                            0.451808  ...                      0.968165   \n",
       "2                            0.013196  ...                      0.997990   \n",
       "3                            0.006137  ...                      0.019716   \n",
       "4                            0.006786  ...                      0.038597   \n",
       "\n",
       "   Principal purpose test  Section 892  Sovereign Immunity  \\\n",
       "0                0.069137     0.607566            0.160612   \n",
       "1                0.093563     0.070805            0.092779   \n",
       "2                0.031619     0.955607            0.186714   \n",
       "3                0.046870     0.016212            0.007157   \n",
       "4                0.026678     0.083684            0.047255   \n",
       "\n",
       "   Tax Governance Framework  Tax audit  Tax compliance  Tax dispute  \\\n",
       "0                  0.706224   0.999475        0.136711     0.998861   \n",
       "1                  0.052677   0.997626        0.285971     0.588859   \n",
       "2                  0.046119   0.375709        0.999481     0.047518   \n",
       "3                  0.005967   0.281438        0.011187     0.982878   \n",
       "4                  0.023218   0.025705        0.016205     0.005099   \n",
       "\n",
       "   Value-added tax (VAT)  Withholding Tax  \n",
       "0               0.409591         0.999835  \n",
       "1               0.963702         0.999671  \n",
       "2               0.252601         0.999754  \n",
       "3               0.039796         0.058485  \n",
       "4               0.919066         0.037344  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recombined_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recombined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombined_results.to_csv(Path(DATA_PATH, \"bert2.1_predictions_round3.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bert-pretrain",
   "language": "python",
   "name": "bert-pretrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
